{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c424ac7",
   "metadata": {},
   "source": [
    "ডেটা প্রিপ্রসেসিংয়ের প্রথম এবং সবচেয়ে গুরুত্বপূর্ণ ধাপ হলো ডেটা পরিষ্কার করা (Data Cleaning)। অপরিষ্কার ডেটা মডেলের পারফরম্যান্সে মারাত্মক প্রভাব ফেলতে পারে।\n",
    "\n",
    "ডেটা ক্লিনিংয়ের প্রধান কাজগুলি হলো:\n",
    "\n",
    "মিসিং ভ্যালু হ্যান্ডলিং (Handling Missing Values)\n",
    "নয়েজি ডেটা হ্যান্ডলিং (Handling Noisy Data)\n",
    "আউটলায়ার হ্যান্ডলিং (Handling Outliers)\n",
    "ডুপ্লিকেট ভ্যালু রিমুভাল (Duplicate Value Removal)\n",
    "চলুন, এই প্রতিটি টপিক বিস্তারিতভাবে আলোচনা করা যাক।\n",
    "\n",
    "1. মিসিং ভ্যালু হ্যান্ডলিং (Handling Missing Values)\n",
    "অনেক সময় ডেটাসেটে কিছু সেলে কোনো ডেটা থাকে না। এগুলোকে \"মিসিং ভ্যালু\" বা \"নাল ভ্যালু\" (NaN - Not a Number) বলা হয়। মিসিং ভ্যালু মডেলের পারফরম্যান্সকে খারাপ করতে পারে, কারণ বেশিরভাগ মেশিন লার্নিং অ্যালগরিদম মিসিং ভ্যালু দিয়ে কাজ করতে পারে না।\n",
    "\n",
    "মিসিং ভ্যালুর কারণ:\n",
    "\n",
    "ডেটা এন্ট্রি ভুল (Data entry errors)\n",
    "ডেটা সংগ্রহের সময় ত্রুটি (Errors during data collection)\n",
    "সিস্টেম ক্র্যাশ (System crashes)\n",
    "প্রতিক্রিয়ার অভাব (Non-response in surveys)\n",
    "ডেটা করাপশন (Data corruption)\n",
    "মিসিং ভ্যালু হ্যান্ডলিংয়ের কৌশল:\n",
    "\n",
    "a.  মিসিং ভ্যালু সহ রো বা কলাম বাদ দেওয়া (Dropping Rows or Columns with Missing Values):\n",
    "* কখন ব্যবহার করবেন:\n",
    "* যখন মিসিং ভ্যালুর সংখ্যা খুব কম হয় (যেমন, মোট ডেটার 1-5% এর কম)।\n",
    "* যদি একটি কলামে (ফিচার) অনেক বেশি মিসিং ভ্যালু থাকে (যেমন, 50% এর বেশি), তাহলে সেই কলামটিই বাদ দিয়ে দেওয়া যুক্তিযুক্ত হতে পারে, কারণ এটি খুব বেশি তথ্য দেয় না।\n",
    "* সুবিধা: সহজ এবং দ্রুত।\n",
    "* অসুবিধা: ডেটার ক্ষতি হতে পারে (ডেটা লস), যা মডেলের পারফরম্যান্সকে প্রভাবিত করতে পারে, বিশেষ করে যখন ডেটাসেট ছোট হয়।"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a501ff",
   "metadata": {},
   "source": [
    "```python\n",
    "# Python উদাহরণ (Pandas লাইব্রেরি ব্যবহার করে)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# একটি উদাহরণ ডেটাফ্রেম তৈরি করি\n",
    "data = {'A': [1, 2, np.nan, 4],\n",
    "        'B': [5, np.nan, 7, 8],\n",
    "        'C': [9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "# মিসিং ভ্যালু সহ রো বাদ দেওয়া\n",
    "df_dropped_rows = df.dropna()\n",
    "print(\"\\nDataFrame after dropping rows with missing values:\\n\", df_dropped_rows)\n",
    "\n",
    "# মিসিং ভ্যালু সহ কলাম বাদ দেওয়া\n",
    "df_dropped_cols = df.dropna(axis=1) # axis=1 কলামের জন্য\n",
    "print(\"\\nDataFrame after dropping columns with missing values:\\n\", df_dropped_cols)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3060dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      A    B   C\n",
      "0  1.0  5.0   9\n",
      "1  2.0  NaN  10\n",
      "2  NaN  7.0  11\n",
      "3  4.0  8.0  12\n",
      "\n",
      "DataFrame after dropping rows with missing values:\n",
      "      A    B   C\n",
      "0  1.0  5.0   9\n",
      "3  4.0  8.0  12\n",
      "\n",
      "DataFrame after dropping columns with missing values:\n",
      "     C\n",
      "0   9\n",
      "1  10\n",
      "2  11\n",
      "3  12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Python উদাহরণ (Pandas লাইব্রেরি ব্যবহার করে)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# একটি উদাহরণ ডেটাফ্রেম তৈরি করি\n",
    "data = {'A': [1, 2, np.nan, 4],\n",
    "        'B': [5, np.nan, 7, 8],\n",
    "        'C': [9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "# মিসিং ভ্যালু সহ রো বাদ দেওয়া\n",
    "df_dropped_rows = df.dropna()\n",
    "print(\"\\nDataFrame after dropping rows with missing values:\\n\", df_dropped_rows)\n",
    "\n",
    "# মিসিং ভ্যালু সহ কলাম বাদ দেওয়া\n",
    "df_dropped_cols = df.dropna(axis=1) # axis=1 কলামের জন্য\n",
    "print(\"\\nDataFrame after dropping columns with missing values:\\n\", df_dropped_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03605723",
   "metadata": {},
   "source": [
    "b.  মিসিং ভ্যালু ইমপিউট করা (Imputing Missing Values):\n",
    "* এই কৌশলটিতে মিসিং ভ্যালুগুলিকে অনুমান করা মান (estimated values) দিয়ে পূরণ করা হয়। এটি ডেটার ক্ষতি এড়াতে সাহায্য করে।\n",
    "* বিভিন্ন কৌশল:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b717234",
   "metadata": {},
   "source": [
    "i.  **মিন (Mean) বা মিডিয়ান (Median) দিয়ে ইমপিউট করা:**\n",
    "        * **মিন:** নিউমেরিক (numerical) ডেটার ক্ষেত্রে যখন ডেটা নর্মালি ডিস্ট্রিবিউটেড (normally distributed) থাকে এবং আউটলায়ার (outliers) কম থাকে।\n",
    "        * **মিডিয়ান:** যখন ডেটাসেটে আউটলায়ার থাকে বা ডেটা স্কিউড (skewed) হয়। মিডিয়ান আউটলায়ার দ্বারা কম প্রভাবিত হয়।\n",
    "        * **কখন ব্যবহার করবেন:** যখন মিসিং ভ্যালুগুলি ডেটার বিতরণকে (distribution) উল্লেখযোগ্যভাবে প্রভাবিত করবে না।\n",
    "        * **সুবিধা:** সহজ এবং কার্যকর।\n",
    "        * **অসুবিধা:** ডেটার ভ্যারিয়েন্স (variance) কমাতে পারে এবং ডেটা সেটকে বায়াসড (biased) করতে পারে।\n",
    "\n",
    "        ```python\n",
    "        # মিন দিয়ে ইমপিউট করা\n",
    "        df_mean_imputed = df.copy()\n",
    "        df_mean_imputed['A'].fillna(df_mean_imputed['A'].mean(), inplace=True)\n",
    "        df_mean_imputed['B'].fillna(df_mean_imputed['B'].mean(), inplace=True)\n",
    "        print(\"\\nDataFrame after mean imputation:\\n\", df_mean_imputed)\n",
    "\n",
    "        # মিডিয়ান দিয়ে ইমপিউট করা\n",
    "        df_median_imputed = df.copy()\n",
    "        df_median_imputed['A'].fillna(df_median_imputed['A'].median(), inplace=True)\n",
    "        df_median_imputed['B'].fillna(df_median_imputed['B'].median(), inplace=True)\n",
    "        print(\"\\nDataFrame after median imputation:\\n\", df_median_imputed)\n",
    "        ```\n",
    "\n",
    "    ii. **মোড (Mode) দিয়ে ইমপিউট করা:**\n",
    "        * **কখন ব্যবহার করবেন:** ক্যাটাগরিক্যাল (categorical) ডেটার ক্ষেত্রে সবচেয়ে বেশি ব্যবহৃত হয়।\n",
    "        * **সুবিধা:** ক্যাটাগরিক্যাল ডেটার জন্য উপযুক্ত।\n",
    "        * **অসুবিধা:** একাধিক মোড থাকলে জটিল হতে পারে।\n",
    "\n",
    "        ```python\n",
    "        # মোড দিয়ে ইমপিউট করা (উদাহরণ হিসেবে একটি ক্যাটাগরিক্যাল কলাম যোগ করি)\n",
    "        data_cat = {'Category': ['A', 'B', np.nan, 'A', 'C', 'B', 'A']}\n",
    "        df_cat = pd.DataFrame(data_cat)\n",
    "        print(\"\\nOriginal Categorical DataFrame:\\n\", df_cat)\n",
    "\n",
    "        # 'Category' কলামের মোড দিয়ে ইমপিউট করা\n",
    "        df_cat_imputed = df_cat.copy()\n",
    "        df_cat_imputed['Category'].fillna(df_cat_imputed['Category'].mode()[0], inplace=True)\n",
    "        print(\"\\nCategorical DataFrame after mode imputation:\\n\", df_cat_imputed)\n",
    "        ```\n",
    "\n",
    "    iii. **পূর্ববর্তী (Previous) বা পরবর্তী (Next) ভ্যালু দিয়ে ইমপিউট করা (Forward Fill/Backward Fill):**\n",
    "        * **কখন ব্যবহার করবেন:** যখন ডেটাসেটে টাইম-সিরিজ ডেটা থাকে বা ডেটার ক্রম (order) গুরুত্বপূর্ণ হয়।\n",
    "        * **Forward Fill (ffill):** আগের নন-মিসিং ভ্যালু দিয়ে পূরণ করা।\n",
    "        * **Backward Fill (bfill):** পরের নন-মিসিং ভ্যালু দিয়ে পূরণ করা।\n",
    "\n",
    "        ```python\n",
    "        # Forward Fill (ffill)\n",
    "        df_ffill = df.copy()\n",
    "        df_ffill.fillna(method='ffill', inplace=True)\n",
    "        print(\"\\nDataFrame after forward fill:\\n\", df_ffill)\n",
    "\n",
    "        # Backward Fill (bfill)\n",
    "        df_bfill = df.copy()\n",
    "        df_bfill.fillna(method='bfill', inplace=True)\n",
    "        print(\"\\nDataFrame after backward fill:\\n\", df_bfill)\n",
    "        ```\n",
    "\n",
    "    iv. **মেশিন লার্নিং মডেল ব্যবহার করে ইমপিউট করা:**\n",
    "        * **কখন ব্যবহার করবেন:** যখন মিসিং ভ্যালুর সংখ্যা বেশি হয় এবং ডেটাসেট তুলনামূলকভাবে বড় হয়।\n",
    "        * **যেমন:** K-Nearest Neighbors (KNN) Imputer, Iterative Imputer (যা রিগ্রেশন মডেল ব্যবহার করে)।\n",
    "        * **সুবিধা:** আরও নির্ভুল অনুমান দিতে পারে।\n",
    "        * **অসুবিধা:** গণনাগতভাবে ব্যয়বহুল (computationally expensive) এবং প্রয়োগ করা তুলনামূলকভাবে জটিল।\n",
    "\n",
    "        ``` python\n",
    "        # KNN Imputer উদাহরণ (scikit-learn ব্যবহার করে)\n",
    "        from sklearn.impute import KNNImputer\n",
    "\n",
    "        data_knn = {'A': [1, 2, np.nan, 4],\n",
    "                    'B': [5, np.nan, 7, 8],\n",
    "                    'C': [9, 10, 11, 12]}\n",
    "        df_knn = pd.DataFrame(data_knn)\n",
    "        print(\"\\nOriginal DataFrame for KNN Imputer:\\n\", df_knn)\n",
    "\n",
    "        imputer = KNNImputer(n_neighbors=2) # 2টি নিকটতম প্রতিবেশী ব্যবহার করে\n",
    "        df_knn_imputed_arr = imputer.fit_transform(df_knn)\n",
    "        df_knn_imputed = pd.DataFrame(df_knn_imputed_arr, columns=df_knn.columns)\n",
    "        print(\"\\nDataFrame after KNN imputation:\\n\", df_knn_imputed)\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd35265",
   "metadata": {},
   "source": [
    "গ্রেট! ডেটা প্রিপ্রসেসিংয়ের দ্বিতীয় গুরুত্বপূর্ণ ধাপ হলো **ডেটা ইন্টিগ্রেশন (Data Integration)**।\n",
    "\n",
    "### ডেটা ইন্টিগ্রেশন কী? (What is Data Integration?)\n",
    "\n",
    "ডেটা ইন্টিগ্রেশন হলো বিভিন্ন উৎস (heterogeneous sources) থেকে প্রাপ্ত ডেটাকে একত্রিত করে একটি একক, ঐক্যবদ্ধ এবং সুসংগত (unified and consistent) ডেটাসেট তৈরি করার প্রক্রিয়া। বাস্তব-বিশ্বে, ডেটা প্রায়শই বিভিন্ন ফরম্যাটে, বিভিন্ন সিস্টেমে এবং বিভিন্ন স্ট্রাকচারে (যেমন - ডেটাবেস, স্প্রেডশীট, টেক্সট ফাইল, ওয়েব সার্ভিসেস) ছড়ানো থাকে। ডেটা ইন্টিগ্রেশনের লক্ষ্য হলো এই বিচ্ছিন্ন ডেটাগুলোকে একসাথে এনে একটি সম্পূর্ণ চিত্র তৈরি করা, যা বিশ্লেষণের জন্য ব্যবহার করা যেতে পারে।\n",
    "\n",
    "**কেন ডেটা ইন্টিগ্রেশন গুরুত্বপূর্ণ?**\n",
    "\n",
    "1.  **সম্পূর্ণ চিত্র (Holistic View):** বিভিন্ন উৎস থেকে ডেটা একত্রিত করে একটি সম্পূর্ণ এবং বিস্তারিত চিত্র পাওয়া যায়, যা শুধুমাত্র একটি উৎস থেকে পাওয়া ডেটা দিয়ে সম্ভব নয়। উদাহরণস্বরূপ, একজন গ্রাহকের লেনদেনের ডেটা একটি ডেটাবেসে এবং তার সোশ্যাল মিডিয়া অ্যাক্টিভিটি অন্য একটি সার্ভিসে থাকতে পারে। ডেটা ইন্টিগ্রেশন এই সব ডেটা একসাথে এনে গ্রাহকের একটি সম্পূর্ণ প্রোফাইল তৈরি করে।\n",
    "2.  **উন্নত বিশ্লেষণ (Improved Analysis):** ডেটা একত্রিত হওয়ার ফলে আরও জটিল এবং ব্যাপক বিশ্লেষণ চালানো যায়, যা গভীর ইনসাইট (insights) প্রদান করে।\n",
    "3.  **সমন্বিত সিদ্ধান্ত গ্রহণ (Coordinated Decision Making):** বিভিন্ন বিভাগের ডেটা একত্রিত হলে সমগ্র প্রতিষ্ঠান জুড়ে আরও অবগত এবং সমন্বিত সিদ্ধান্ত গ্রহণ করা সম্ভব হয়।\n",
    "4.  **রিডানডেন্সি হ্রাস (Reduced Redundancy):** ইন্টিগ্রেশন প্রক্রিয়ায় প্রায়শই ডুপ্লিকেট ডেটা শনাক্ত করে তা অপসারণ করা হয়, যা ডেটার রিডানডেন্সি কমায় এবং ডেটা স্টোরেজ ও প্রক্রিয়াকরণের দক্ষতা বাড়ায়।\n",
    "5.  **ডেটা গুণমান বৃদ্ধি (Improved Data Quality):** ডেটা ইন্টিগ্রেশনের সময় ডেটার অসঙ্গতি (inconsistencies) এবং ত্রুটি (errors) সনাক্ত ও সমাধান করা হয়, যা সামগ্রিক ডেটা গুণমান উন্নত করে।\n",
    "\n",
    "---\n",
    "\n",
    "### ডেটা ইন্টিগ্রেশনের প্রধান চ্যালেঞ্জসমূহ (Major Challenges in Data Integration)\n",
    "\n",
    "ডেটা ইন্টিগ্রেশনের সময় বেশ কিছু চ্যালেঞ্জের সম্মুখীন হতে হয়:\n",
    "\n",
    "1.  **এন্ট্রি কনফ্লিক্টস এবং ডেটা রিডানডেন্সি (Entity Conflicts and Data Redundancy):**\n",
    "    * বিভিন্ন ডেটা উৎসে একই এন্ট্রির জন্য ভিন্ন ভিন্ন অ্যাট্রিবিউট নেমিং (যেমন, 'গ্রাহক আইডি' বনাম 'কাস্টমার আইডি') বা ভিন্ন ভিন্ন ডেটা ফরম্যাট থাকতে পারে।\n",
    "    * একই ডেটা একাধিক উৎসে থাকতে পারে (ডুপ্লিকেট ডেটা), যা অসঙ্গতি তৈরি করতে পারে।\n",
    "\n",
    "2.  **স্কিমা ইন্টিগ্রেশন (Schema Integration):**\n",
    "    * বিভিন্ন ডেটা উৎসের ডেটাবেস স্কিমা (table structures, column names, data types) ভিন্ন হতে পারে। এগুলোকে একটি সুসংগত গ্লোবাল স্কিমায় একত্রিত করা কঠিন হতে পারে।\n",
    "    * **সমস্যার উদাহরণ:**\n",
    "        * **এন্ট্রি আইডেন্টিফিকেশন (Entity Identification):** একই বাস্তব-বিশ্বের সত্তার (যেমন, একজন গ্রাহক) জন্য বিভিন্ন উৎসে ভিন্ন ভিন্ন আইডি থাকতে পারে।\n",
    "        * **ডেটা ভ্যালু কনফ্লিক্টস (Data Value Conflicts):** একই অ্যাট্রিবিউটের জন্য বিভিন্ন উৎসে ভিন্ন মান থাকতে পারে (যেমন, একজন গ্রাহকের বয়স একটি উৎসে 30 এবং অন্যটিতে 31)।\n",
    "        * **ডেটা টাইপ কনফ্লিক্টস (Data Type Conflicts):** একই অ্যাট্রিবিউটের জন্য বিভিন্ন উৎসে ভিন্ন ডেটা টাইপ থাকতে পারে (যেমন, ফোন নম্বর একটিতে সংখ্যা এবং অন্যটিতে টেক্সট)।\n",
    "\n",
    "3.  **কনসিস্টেন্সি ইস্যু (Consistency Issues):**\n",
    "    * ডেটা একত্রিত করার সময় ডেটার মান, ডেটা টাইপ এবং ফরম্যাটের মধ্যে অসঙ্গতি দেখা দিতে পারে।\n",
    "\n",
    "---\n",
    "\n",
    "### ডেটা ইন্টিগ্রেশনের প্রধান কৌশলসমূহ (Key Techniques in Data Integration)\n",
    "\n",
    "ডেটা ইন্টিগ্রেশন সাধারণত ETL (Extract, Transform, Load) প্রক্রিয়ার মাধ্যমে সম্পন্ন হয়। তবে, মেশিন লার্নিংয়ের ডেটা প্রিপ্রসেসিংয়ের প্রসঙ্গে আমরা প্রধানত ডেটা একত্রিত করার দিকটি দেখবো।\n",
    "\n",
    "1.  **ডেটা একত্রিত করা (Combining Data):**\n",
    "\n",
    "    a.  **মার্জ (Merge) / জয়েন (Join):**\n",
    "        * **কখন ব্যবহার করবেন:** যখন আপনার কাছে একাধিক ডেটাফ্রেম বা টেবিল থাকে এবং আপনি একটি বা একাধিক সাধারণ কলামের (key) উপর ভিত্তি করে সেগুলিকে একত্রিত করতে চান। এটি ডেটাবেস টেবিল জয়েন করার মতোই।\n",
    "        * **উদাহরণ:** গ্রাহকের ব্যক্তিগত তথ্য (গ্রাহক আইডি, নাম) একটি টেবিল এবং তার কেনাকাটার তথ্য (গ্রাহক আইডি, পণ্যের নাম, দাম) অন্য একটি টেবিলে থাকলে, 'গ্রাহক আইডি' ব্যবহার করে দুটি টেবিলকে মার্জ করা যায়।\n",
    "        * **প্রকারভেদ:** Inner Join, Left Join, Right Join, Outer Join (Pandas এ `pd.merge()` ব্যবহার করা হয়)।\n",
    "\n",
    "        ```python\n",
    "        # Python উদাহরণ (Pandas ব্যবহার করে)\n",
    "        import pandas as pd\n",
    "\n",
    "        # ডেটাফ্রেম 1\n",
    "        df1 = pd.DataFrame({'customer_id': [1, 2, 3, 4],\n",
    "                            'name': ['Alice', 'Bob', 'Charlie', 'David']})\n",
    "\n",
    "        # ডেটাফ্রেম 2\n",
    "        df2 = pd.DataFrame({'customer_id': [1, 3, 5, 2],\n",
    "                            'order_id': ['A1', 'C1', 'E1', 'B1'],\n",
    "                            'amount': [100, 200, 300, 150]})\n",
    "\n",
    "        print(\"DataFrame 1 (Customer Info):\\n\", df1)\n",
    "        print(\"\\nDataFrame 2 (Order Info):\\n\", df2)\n",
    "\n",
    "        # customer_id কলামের উপর ভিত্তি করে Inner Join\n",
    "        # শুধুমাত্র যে customer_id উভয় DataFrame-এ আছে, সেগুলো দেখাবে।\n",
    "        merged_df_inner = pd.merge(df1, df2, on='customer_id', how='inner')\n",
    "        print(\"\\nMerged DataFrame (Inner Join):\\n\", merged_df_inner)\n",
    "\n",
    "        # Left Join (df1 এর সব ডেটা থাকবে, df2 থেকে matching data আসবে)\n",
    "        merged_df_left = pd.merge(df1, df2, on='customer_id', how='left')\n",
    "        print(\"\\nMerged DataFrame (Left Join):\\n\", merged_df_left)\n",
    "        ```\n",
    "\n",
    "    b.  **কনক্যাটেনেশন (Concatenation):**\n",
    "        * **কখন ব্যবহার করবেন:** যখন আপনার কাছে একই স্ট্রাকচার (একই কলাম হেডার) সহ একাধিক ডেটাফ্রেম থাকে এবং আপনি সেগুলিকে সারি (rows) বরাবর বা কলাম (columns) বরাবর একত্রিত করতে চান।\n",
    "        * **উদাহরণ:** বিভিন্ন মাসের বিক্রয় ডেটা (যেখানে কলামগুলি একই) একত্রিত করা।\n",
    "        * `pd.concat()` ব্যবহার করা হয়।\n",
    "\n",
    "        ```python\n",
    "        # কনক্যাটেনেশন উদাহরণ\n",
    "        df_jan = pd.DataFrame({'product': ['A', 'B'], 'sales': [100, 150]})\n",
    "        df_feb = pd.DataFrame({'product': ['C', 'A'], 'sales': [200, 120]})\n",
    "\n",
    "        print(\"\\nSales Jan:\\n\", df_jan)\n",
    "        print(\"\\nSales Feb:\\n\", df_feb)\n",
    "\n",
    "        # সারি বরাবর কনক্যাটেনেশন\n",
    "        all_sales = pd.concat([df_jan, df_feb], axis=0) # axis=0 rows এর জন্য\n",
    "        print(\"\\nAll Sales (Concatenated by rows):\\n\", all_sales)\n",
    "\n",
    "        # কলাম বরাবর কনক্যাটেনেশন (যদি কলামগুলি একই অর্থ বহন করে এবং প্রতিটি সারিতে matching data থাকে)\n",
    "        # সাধারণত এটি Join এর মতো কাজ করে না যদি index match না করে।\n",
    "        # এখানে উদাহরণ হিসেবে দেখানোর জন্য এমন কলাম নেই যা পাশাপাশি যুক্ত করা যায়।\n",
    "        # কিন্তু যদি আপনি ডেটাফ্রেমগুলোকে পাশাপাশি যুক্ত করতে চান:\n",
    "        # pd.concat([df_a, df_b], axis=1)\n",
    "        ```\n",
    "\n",
    "2.  **স্কিমা ইন্টিগ্রেশন (Schema Integration):**\n",
    "    * এটি বিভিন্ন উৎসের ডেটাবেস স্কিমা বা ডেটাফ্রেমের স্ট্রাকচারকে একটি সামঞ্জস্যপূর্ণ ফরম্যাটে নিয়ে আসার প্রক্রিয়া।\n",
    "    * **নামের অসঙ্গতি ঠিক করা (Handling Naming Conflicts):**\n",
    "        * বিভিন্ন উৎসে একই তথ্যের জন্য ভিন্ন ভিন্ন অ্যাট্রিবিউট নাম থাকতে পারে (যেমন `Cust_ID`, `Customer ID`, `CustomerID`)। এই নামগুলোকে একটি সাধারণ এবং মানসম্মত নামে পরিবর্তন করা হয়।\n",
    "        * **উপায়:** কলাম রিনেম (rename) করা।\n",
    "\n",
    "        ```python\n",
    "        # কলাম রিনেম উদাহরণ\n",
    "        df_inconsistent = pd.DataFrame({'Cust_ID': [1, 2], 'CustName': ['A', 'B']})\n",
    "        print(\"\\nInconsistent DataFrame:\\n\", df_inconsistent)\n",
    "\n",
    "        df_consistent = df_inconsistent.rename(columns={'Cust_ID': 'CustomerID', 'CustName': 'CustomerName'})\n",
    "        print(\"\\nConsistent DataFrame (after renaming columns):\\n\", df_consistent)\n",
    "        ```\n",
    "\n",
    "    * **ডেটা টাইপ সামঞ্জস্যপূর্ণ করা (Ensuring Data Type Consistency):**\n",
    "        * বিভিন্ন উৎসে একই অ্যাট্রিবিউটের জন্য ভিন্ন ডেটা টাইপ থাকতে পারে (যেমন, 'বয়স' একটিতে সংখ্যা এবং অন্যটিতে টেক্সট)। ডেটা টাইপগুলিকে একটি সাধারণ টাইপে রূপান্তর করা হয়।\n",
    "        * **উপায়:** ডেটা টাইপ কাস্ট (cast) করা।\n",
    "\n",
    "        ```python\n",
    "        # ডেটা টাইপ কনভার্সন উদাহরণ\n",
    "        df_dtype = pd.DataFrame({'ID': ['1', '2', '3'], 'Value': ['10', '20', '30']})\n",
    "        print(\"\\nOriginal DataFrame (with inconsistent data types):\\n\", df_dtype)\n",
    "        print(df_dtype.dtypes)\n",
    "\n",
    "        df_dtype['ID'] = df_dtype['ID'].astype(int)\n",
    "        df_dtype['Value'] = df_dtype['Value'].astype(float)\n",
    "        print(\"\\nDataFrame (after converting data types):\\n\", df_dtype)\n",
    "        print(df_dtype.dtypes)\n",
    "        ```\n",
    "\n",
    "3.  **ডেটা ভ্যালু কনফ্লিক্টস হ্যান্ডলিং (Handling Data Value Conflicts):**\n",
    "    * যদি একই অ্যাট্রিবিউটের জন্য বিভিন্ন উৎসে ভিন্ন মান থাকে (যেমন, একজন ব্যক্তির ঠিকানা দুটি ভিন্ন সিস্টেমে সামান্য ভিন্ন), তবে কোনটি সঠিক বা সবচেয়ে আপডেটেড তা সিদ্ধান্ত নিতে হয়।\n",
    "    * **উপায়:**\n",
    "        * **ডেটা প্রায়োরিটি (Data Priority):** একটি নির্দিষ্ট উৎসের ডেটাকে অন্য উৎসের চেয়ে বেশি অগ্রাধিকার দেওয়া।\n",
    "        * **সর্বশেষ আপডেট (Latest Update):** সবচেয়ে সাম্প্রতিক আপডেট হওয়া ডেটাকে বেছে নেওয়া।\n",
    "        * **ভোট/অ্যাভারেজ (Voting/Averaging):** একাধিক উৎস থেকে পাওয়া মানগুলির গড় নেওয়া।\n",
    "        * **ম্যানুয়াল ইন্টারভেনশন (Manual Intervention):** কিছু ক্ষেত্রে ম্যানুয়ালি যাচাই করা প্রয়োজন হতে পারে।\n",
    "\n",
    "4.  **ডেটা রিডানডেন্সি এবং ইনকনসিস্টেন্সি হ্যান্ডলিং (Handling Data Redundancy and Inconsistency):**\n",
    "    * ডেটা ইন্টিগ্রেশনের সময় প্রায়শই ডুপ্লিকেট ডেটা বা অসঙ্গতিপূর্ণ ডেটা তৈরি হয়।\n",
    "    * **ডুপ্লিকেট রিমুভাল:** আগেই আলোচনা করা হয়েছে (ডেটা ক্লিনিং এর অংশ)।\n",
    "    * **ইনকনসিস্টেন্সি রেজোলিউশন:** ডেটা ক্লিনিং এবং ইন্টিগ্রেশন রুলস ব্যবহার করে অসঙ্গতিপূর্ণ ডেটা ঠিক করা।\n",
    "\n",
    "---\n",
    "\n",
    "**ডেটা ইন্টিগ্রেশনের কার্যকরী অনুশীলন (Best Practices for Data Integration):**\n",
    "\n",
    "* **ডেটা প্রোফাইলিং (Data Profiling):** ইন্টিগ্রেশনের আগে প্রতিটি ডেটা উৎসের ডেটা স্ট্রাকচার, গুণমান এবং বিষয়বস্তু পুঙ্খানুপুঙ্খভাবে বিশ্লেষণ করা।\n",
    "* **মেটাডেটা ম্যানেজমেন্ট (Metadata Management):** ডেটা উৎস, স্কিমা এবং ইন্টিগ্রেশন নিয়মাবলী সম্পর্কে বিস্তারিত মেটাডেটা বজায় রাখা।\n",
    "* **ডেটা ট্রান্সফরমেশন টুলস (Data Transformation Tools):** ETL টুলস বা প্রোগ্রামিং লাইব্রেরি (যেমন Pandas, PySpark) ব্যবহার করে ডেটা ট্রান্সফরমেশন এবং ইন্টিগ্রেশন প্রক্রিয়াকে স্বয়ংক্রিয় করা।\n",
    "* **ইন্টিগ্রেশন স্ট্র্যাটেজি (Integration Strategy):** আপনার প্রয়োজনের উপর ভিত্তি করে সঠিক ইন্টিগ্রেশন স্ট্র্যাটেজি (যেমন ডেটা ওয়্যারহাউস, ডেটা লেক, ফেডারেটেড ডেটাবেস) নির্বাচন করা।\n",
    "\n",
    "ডেটা ইন্টিগ্রেশন মেশিন লার্নিং মডেলের জন্য একটি সম্পূর্ণ এবং নির্ভরযোগ্য ডেটাসেট নিশ্চিত করতে সাহায্য করে। এটি জটিল কিন্তু একটি অপরিহার্য প্রক্রিয়া।\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2469a20",
   "metadata": {},
   "source": [
    "গ্রেট! ডেটা প্রিপ্রসেসিংয়ের দ্বিতীয় গুরুত্বপূর্ণ ধাপ হলো **ডেটা ইন্টিগ্রেশন (Data Integration)**।\n",
    "\n",
    "### ডেটা ইন্টিগ্রেশন কী? (What is Data Integration?)\n",
    "\n",
    "ডেটা ইন্টিগ্রেশন হলো বিভিন্ন উৎস (heterogeneous sources) থেকে প্রাপ্ত ডেটাকে একত্রিত করে একটি একক, ঐক্যবদ্ধ এবং সুসংগত (unified and consistent) ডেটাসেট তৈরি করার প্রক্রিয়া। বাস্তব-বিশ্বে, ডেটা প্রায়শই বিভিন্ন ফরম্যাটে, বিভিন্ন সিস্টেমে এবং বিভিন্ন স্ট্রাকচারে (যেমন - ডেটাবেস, স্প্রেডশীট, টেক্সট ফাইল, ওয়েব সার্ভিসেস) ছড়ানো থাকে। ডেটা ইন্টিগ্রেশনের লক্ষ্য হলো এই বিচ্ছিন্ন ডেটাগুলোকে একসাথে এনে একটি সম্পূর্ণ চিত্র তৈরি করা, যা বিশ্লেষণের জন্য ব্যবহার করা যেতে পারে।\n",
    "\n",
    "**কেন ডেটা ইন্টিগ্রেশন গুরুত্বপূর্ণ?**\n",
    "\n",
    "1.  **সম্পূর্ণ চিত্র (Holistic View):** বিভিন্ন উৎস থেকে ডেটা একত্রিত করে একটি সম্পূর্ণ এবং বিস্তারিত চিত্র পাওয়া যায়, যা শুধুমাত্র একটি উৎস থেকে পাওয়া ডেটা দিয়ে সম্ভব নয়। উদাহরণস্বরূপ, একজন গ্রাহকের লেনদেনের ডেটা একটি ডেটাবেসে এবং তার সোশ্যাল মিডিয়া অ্যাক্টিভিটি অন্য একটি সার্ভিসে থাকতে পারে। ডেটা ইন্টিগ্রেশন এই সব ডেটা একসাথে এনে গ্রাহকের একটি সম্পূর্ণ প্রোফাইল তৈরি করে।\n",
    "2.  **উন্নত বিশ্লেষণ (Improved Analysis):** ডেটা একত্রিত হওয়ার ফলে আরও জটিল এবং ব্যাপক বিশ্লেষণ চালানো যায়, যা গভীর ইনসাইট (insights) প্রদান করে।\n",
    "3.  **সমন্বিত সিদ্ধান্ত গ্রহণ (Coordinated Decision Making):** বিভিন্ন বিভাগের ডেটা একত্রিত হলে সমগ্র প্রতিষ্ঠান জুড়ে আরও অবগত এবং সমন্বিত সিদ্ধান্ত গ্রহণ করা সম্ভব হয়।\n",
    "4.  **রিডানডেন্সি হ্রাস (Reduced Redundancy):** ইন্টিগ্রেশন প্রক্রিয়ায় প্রায়শই ডুপ্লিকেট ডেটা শনাক্ত করে তা অপসারণ করা হয়, যা ডেটার রিডানডেন্সি কমায় এবং ডেটা স্টোরেজ ও প্রক্রিয়াকরণের দক্ষতা বাড়ায়।\n",
    "5.  **ডেটা গুণমান বৃদ্ধি (Improved Data Quality):** ডেটা ইন্টিগ্রেশনের সময় ডেটার অসঙ্গতি (inconsistencies) এবং ত্রুটি (errors) সনাক্ত ও সমাধান করা হয়, যা সামগ্রিক ডেটা গুণমান উন্নত করে।\n",
    "\n",
    "---\n",
    "\n",
    "### ডেটা ইন্টিগ্রেশনের প্রধান চ্যালেঞ্জসমূহ (Major Challenges in Data Integration)\n",
    "\n",
    "ডেটা ইন্টিগ্রেশনের সময় বেশ কিছু চ্যালেঞ্জের সম্মুখীন হতে হয়:\n",
    "\n",
    "1.  **এন্ট্রি কনফ্লিক্টস এবং ডেটা রিডানডেন্সি (Entity Conflicts and Data Redundancy):**\n",
    "    * বিভিন্ন ডেটা উৎসে একই এন্ট্রির জন্য ভিন্ন ভিন্ন অ্যাট্রিবিউট নেমিং (যেমন, 'গ্রাহক আইডি' বনাম 'কাস্টমার আইডি') বা ভিন্ন ভিন্ন ডেটা ফরম্যাট থাকতে পারে।\n",
    "    * একই ডেটা একাধিক উৎসে থাকতে পারে (ডুপ্লিকেট ডেটা), যা অসঙ্গতি তৈরি করতে পারে।\n",
    "\n",
    "2.  **স্কিমা ইন্টিগ্রেশন (Schema Integration):**\n",
    "    * বিভিন্ন ডেটা উৎসের ডেটাবেস স্কিমা (table structures, column names, data types) ভিন্ন হতে পারে। এগুলোকে একটি সুসংগত গ্লোবাল স্কিমায় একত্রিত করা কঠিন হতে পারে।\n",
    "    * **সমস্যার উদাহরণ:**\n",
    "        * **এন্ট্রি আইডেন্টিফিকেশন (Entity Identification):** একই বাস্তব-বিশ্বের সত্তার (যেমন, একজন গ্রাহক) জন্য বিভিন্ন উৎসে ভিন্ন ভিন্ন আইডি থাকতে পারে।\n",
    "        * **ডেটা ভ্যালু কনফ্লিক্টস (Data Value Conflicts):** একই অ্যাট্রিবিউটের জন্য বিভিন্ন উৎসে ভিন্ন মান থাকতে পারে (যেমন, একজন গ্রাহকের বয়স একটি উৎসে 30 এবং অন্যটিতে 31)।\n",
    "        * **ডেটা টাইপ কনফ্লিক্টস (Data Type Conflicts):** একই অ্যাট্রিবিউটের জন্য বিভিন্ন উৎসে ভিন্ন ডেটা টাইপ থাকতে পারে (যেমন, ফোন নম্বর একটিতে সংখ্যা এবং অন্যটিতে টেক্সট)।\n",
    "\n",
    "3.  **কনসিস্টেন্সি ইস্যু (Consistency Issues):**\n",
    "    * ডেটা একত্রিত করার সময় ডেটার মান, ডেটা টাইপ এবং ফরম্যাটের মধ্যে অসঙ্গতি দেখা দিতে পারে।\n",
    "\n",
    "---\n",
    "\n",
    "### ডেটা ইন্টিগ্রেশনের প্রধান কৌশলসমূহ (Key Techniques in Data Integration)\n",
    "\n",
    "ডেটা ইন্টিগ্রেশন সাধারণত ETL (Extract, Transform, Load) প্রক্রিয়ার মাধ্যমে সম্পন্ন হয়। তবে, মেশিন লার্নিংয়ের ডেটা প্রিপ্রসেসিংয়ের প্রসঙ্গে আমরা প্রধানত ডেটা একত্রিত করার দিকটি দেখবো।\n",
    "\n",
    "1.  **ডেটা একত্রিত করা (Combining Data):**\n",
    "\n",
    "    a.  **মার্জ (Merge) / জয়েন (Join):**\n",
    "        * **কখন ব্যবহার করবেন:** যখন আপনার কাছে একাধিক ডেটাফ্রেম বা টেবিল থাকে এবং আপনি একটি বা একাধিক সাধারণ কলামের (key) উপর ভিত্তি করে সেগুলিকে একত্রিত করতে চান। এটি ডেটাবেস টেবিল জয়েন করার মতোই।\n",
    "        * **উদাহরণ:** গ্রাহকের ব্যক্তিগত তথ্য (গ্রাহক আইডি, নাম) একটি টেবিল এবং তার কেনাকাটার তথ্য (গ্রাহক আইডি, পণ্যের নাম, দাম) অন্য একটি টেবিলে থাকলে, 'গ্রাহক আইডি' ব্যবহার করে দুটি টেবিলকে মার্জ করা যায়।\n",
    "        * **প্রকারভেদ:** Inner Join, Left Join, Right Join, Outer Join (Pandas এ `pd.merge()` ব্যবহার করা হয়)।\n",
    "\n",
    "        ```python\n",
    "        # Python উদাহরণ (Pandas ব্যবহার করে)\n",
    "        import pandas as pd\n",
    "\n",
    "        # ডেটাফ্রেম 1\n",
    "        df1 = pd.DataFrame({'customer_id': [1, 2, 3, 4],\n",
    "                            'name': ['Alice', 'Bob', 'Charlie', 'David']})\n",
    "\n",
    "        # ডেটাফ্রেম 2\n",
    "        df2 = pd.DataFrame({'customer_id': [1, 3, 5, 2],\n",
    "                            'order_id': ['A1', 'C1', 'E1', 'B1'],\n",
    "                            'amount': [100, 200, 300, 150]})\n",
    "\n",
    "        print(\"DataFrame 1 (Customer Info):\\n\", df1)\n",
    "        print(\"\\nDataFrame 2 (Order Info):\\n\", df2)\n",
    "\n",
    "        # customer_id কলামের উপর ভিত্তি করে Inner Join\n",
    "        # শুধুমাত্র যে customer_id উভয় DataFrame-এ আছে, সেগুলো দেখাবে।\n",
    "        merged_df_inner = pd.merge(df1, df2, on='customer_id', how='inner')\n",
    "        print(\"\\nMerged DataFrame (Inner Join):\\n\", merged_df_inner)\n",
    "\n",
    "        # Left Join (df1 এর সব ডেটা থাকবে, df2 থেকে matching data আসবে)\n",
    "        merged_df_left = pd.merge(df1, df2, on='customer_id', how='left')\n",
    "        print(\"\\nMerged DataFrame (Left Join):\\n\", merged_df_left)\n",
    "        ```\n",
    "\n",
    "    b.  **কনক্যাটেনেশন (Concatenation):**\n",
    "        * **কখন ব্যবহার করবেন:** যখন আপনার কাছে একই স্ট্রাকচার (একই কলাম হেডার) সহ একাধিক ডেটাফ্রেম থাকে এবং আপনি সেগুলিকে সারি (rows) বরাবর বা কলাম (columns) বরাবর একত্রিত করতে চান।\n",
    "        * **উদাহরণ:** বিভিন্ন মাসের বিক্রয় ডেটা (যেখানে কলামগুলি একই) একত্রিত করা।\n",
    "        * `pd.concat()` ব্যবহার করা হয়।\n",
    "\n",
    "        ```python\n",
    "        # কনক্যাটেনেশন উদাহরণ\n",
    "        df_jan = pd.DataFrame({'product': ['A', 'B'], 'sales': [100, 150]})\n",
    "        df_feb = pd.DataFrame({'product': ['C', 'A'], 'sales': [200, 120]})\n",
    "\n",
    "        print(\"\\nSales Jan:\\n\", df_jan)\n",
    "        print(\"\\nSales Feb:\\n\", df_feb)\n",
    "\n",
    "        # সারি বরাবর কনক্যাটেনেশন\n",
    "        all_sales = pd.concat([df_jan, df_feb], axis=0) # axis=0 rows এর জন্য\n",
    "        print(\"\\nAll Sales (Concatenated by rows):\\n\", all_sales)\n",
    "\n",
    "        # কলাম বরাবর কনক্যাটেনেশন (যদি কলামগুলি একই অর্থ বহন করে এবং প্রতিটি সারিতে matching data থাকে)\n",
    "        # সাধারণত এটি Join এর মতো কাজ করে না যদি index match না করে।\n",
    "        # এখানে উদাহরণ হিসেবে দেখানোর জন্য এমন কলাম নেই যা পাশাপাশি যুক্ত করা যায়।\n",
    "        # কিন্তু যদি আপনি ডেটাফ্রেমগুলোকে পাশাপাশি যুক্ত করতে চান:\n",
    "        # pd.concat([df_a, df_b], axis=1)\n",
    "        ```\n",
    "\n",
    "2.  **স্কিমা ইন্টিগ্রেশন (Schema Integration):**\n",
    "    * এটি বিভিন্ন উৎসের ডেটাবেস স্কিমা বা ডেটাফ্রেমের স্ট্রাকচারকে একটি সামঞ্জস্যপূর্ণ ফরম্যাটে নিয়ে আসার প্রক্রিয়া।\n",
    "    * **নামের অসঙ্গতি ঠিক করা (Handling Naming Conflicts):**\n",
    "        * বিভিন্ন উৎসে একই তথ্যের জন্য ভিন্ন ভিন্ন অ্যাট্রিবিউট নাম থাকতে পারে (যেমন `Cust_ID`, `Customer ID`, `CustomerID`)। এই নামগুলোকে একটি সাধারণ এবং মানসম্মত নামে পরিবর্তন করা হয়।\n",
    "        * **উপায়:** কলাম রিনেম (rename) করা।\n",
    "\n",
    "        ```python\n",
    "        # কলাম রিনেম উদাহরণ\n",
    "        df_inconsistent = pd.DataFrame({'Cust_ID': [1, 2], 'CustName': ['A', 'B']})\n",
    "        print(\"\\nInconsistent DataFrame:\\n\", df_inconsistent)\n",
    "\n",
    "        df_consistent = df_inconsistent.rename(columns={'Cust_ID': 'CustomerID', 'CustName': 'CustomerName'})\n",
    "        print(\"\\nConsistent DataFrame (after renaming columns):\\n\", df_consistent)\n",
    "        ```\n",
    "\n",
    "    * **ডেটা টাইপ সামঞ্জস্যপূর্ণ করা (Ensuring Data Type Consistency):**\n",
    "        * বিভিন্ন উৎসে একই অ্যাট্রিবিউটের জন্য ভিন্ন ডেটা টাইপ থাকতে পারে (যেমন, 'বয়স' একটিতে সংখ্যা এবং অন্যটিতে টেক্সট)। ডেটা টাইপগুলিকে একটি সাধারণ টাইপে রূপান্তর করা হয়।\n",
    "        * **উপায়:** ডেটা টাইপ কাস্ট (cast) করা।\n",
    "\n",
    "        ```python\n",
    "        # ডেটা টাইপ কনভার্সন উদাহরণ\n",
    "        df_dtype = pd.DataFrame({'ID': ['1', '2', '3'], 'Value': ['10', '20', '30']})\n",
    "        print(\"\\nOriginal DataFrame (with inconsistent data types):\\n\", df_dtype)\n",
    "        print(df_dtype.dtypes)\n",
    "\n",
    "        df_dtype['ID'] = df_dtype['ID'].astype(int)\n",
    "        df_dtype['Value'] = df_dtype['Value'].astype(float)\n",
    "        print(\"\\nDataFrame (after converting data types):\\n\", df_dtype)\n",
    "        print(df_dtype.dtypes)\n",
    "        ```\n",
    "\n",
    "3.  **ডেটা ভ্যালু কনফ্লিক্টস হ্যান্ডলিং (Handling Data Value Conflicts):**\n",
    "    * যদি একই অ্যাট্রিবিউটের জন্য বিভিন্ন উৎসে ভিন্ন মান থাকে (যেমন, একজন ব্যক্তির ঠিকানা দুটি ভিন্ন সিস্টেমে সামান্য ভিন্ন), তবে কোনটি সঠিক বা সবচেয়ে আপডেটেড তা সিদ্ধান্ত নিতে হয়।\n",
    "    * **উপায়:**\n",
    "        * **ডেটা প্রায়োরিটি (Data Priority):** একটি নির্দিষ্ট উৎসের ডেটাকে অন্য উৎসের চেয়ে বেশি অগ্রাধিকার দেওয়া।\n",
    "        * **সর্বশেষ আপডেট (Latest Update):** সবচেয়ে সাম্প্রতিক আপডেট হওয়া ডেটাকে বেছে নেওয়া।\n",
    "        * **ভোট/অ্যাভারেজ (Voting/Averaging):** একাধিক উৎস থেকে পাওয়া মানগুলির গড় নেওয়া।\n",
    "        * **ম্যানুয়াল ইন্টারভেনশন (Manual Intervention):** কিছু ক্ষেত্রে ম্যানুয়ালি যাচাই করা প্রয়োজন হতে পারে।\n",
    "\n",
    "4.  **ডেটা রিডানডেন্সি এবং ইনকনসিস্টেন্সি হ্যান্ডলিং (Handling Data Redundancy and Inconsistency):**\n",
    "    * ডেটা ইন্টিগ্রেশনের সময় প্রায়শই ডুপ্লিকেট ডেটা বা অসঙ্গতিপূর্ণ ডেটা তৈরি হয়।\n",
    "    * **ডুপ্লিকেট রিমুভাল:** আগেই আলোচনা করা হয়েছে (ডেটা ক্লিনিং এর অংশ)।\n",
    "    * **ইনকনসিস্টেন্সি রেজোলিউশন:** ডেটা ক্লিনিং এবং ইন্টিগ্রেশন রুলস ব্যবহার করে অসঙ্গতিপূর্ণ ডেটা ঠিক করা।\n",
    "\n",
    "---\n",
    "\n",
    "**ডেটা ইন্টিগ্রেশনের কার্যকরী অনুশীলন (Best Practices for Data Integration):**\n",
    "\n",
    "* **ডেটা প্রোফাইলিং (Data Profiling):** ইন্টিগ্রেশনের আগে প্রতিটি ডেটা উৎসের ডেটা স্ট্রাকচার, গুণমান এবং বিষয়বস্তু পুঙ্খানুপুঙ্খভাবে বিশ্লেষণ করা।\n",
    "* **মেটাডেটা ম্যানেজমেন্ট (Metadata Management):** ডেটা উৎস, স্কিমা এবং ইন্টিগ্রেশন নিয়মাবলী সম্পর্কে বিস্তারিত মেটাডেটা বজায় রাখা।\n",
    "* **ডেটা ট্রান্সফরমেশন টুলস (Data Transformation Tools):** ETL টুলস বা প্রোগ্রামিং লাইব্রেরি (যেমন Pandas, PySpark) ব্যবহার করে ডেটা ট্রান্সফরমেশন এবং ইন্টিগ্রেশন প্রক্রিয়াকে স্বয়ংক্রিয় করা।\n",
    "* **ইন্টিগ্রেশন স্ট্র্যাটেজি (Integration Strategy):** আপনার প্রয়োজনের উপর ভিত্তি করে সঠিক ইন্টিগ্রেশন স্ট্র্যাটেজি (যেমন ডেটা ওয়্যারহাউস, ডেটা লেক, ফেডারেটেড ডেটাবেস) নির্বাচন করা।\n",
    "\n",
    "ডেটা ইন্টিগ্রেশন মেশিন লার্নিং মডেলের জন্য একটি সম্পূর্ণ এবং নির্ভরযোগ্য ডেটাসেট নিশ্চিত করতে সাহায্য করে। এটি জটিল কিন্তু একটি অপরিহার্য প্রক্রিয়া।\n",
    "\n",
    "আপনার কি ডেটা ইন্টিগ্রেশনের কোনো নির্দিষ্ট অংশ নিয়ে আরও বিস্তারিত জানার প্রয়োজন আছে, অথবা কোনো প্রশ্ন আছে? এরপর আমরা **ডেটা ট্রান্সফরমেশন (Data Transformation)** নিয়ে আলোচনা করব।"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22155c8",
   "metadata": {},
   "source": [
    "গ্রেট! ডেটা প্রিপ্রসেসিংয়ের দ্বিতীয় গুরুত্বপূর্ণ ধাপ হলো **ডেটা ইন্টিগ্রেশন (Data Integration)**।\n",
    "\n",
    "### ডেটা ইন্টিগ্রেশন কী? (What is Data Integration?)\n",
    "\n",
    "ডেটা ইন্টিগ্রেশন হলো বিভিন্ন উৎস (heterogeneous sources) থেকে প্রাপ্ত ডেটাকে একত্রিত করে একটি একক, ঐক্যবদ্ধ এবং সুসংগত (unified and consistent) ডেটাসেট তৈরি করার প্রক্রিয়া। বাস্তব-বিশ্বে, ডেটা প্রায়শই বিভিন্ন ফরম্যাটে, বিভিন্ন সিস্টেমে এবং বিভিন্ন স্ট্রাকচারে (যেমন - ডেটাবেস, স্প্রেডশীট, টেক্সট ফাইল, ওয়েব সার্ভিসেস) ছড়ানো থাকে। ডেটা ইন্টিগ্রেশনের লক্ষ্য হলো এই বিচ্ছিন্ন ডেটাগুলোকে একসাথে এনে একটি সম্পূর্ণ চিত্র তৈরি করা, যা বিশ্লেষণের জন্য ব্যবহার করা যেতে পারে।\n",
    "\n",
    "**কেন ডেটা ইন্টিগ্রেশন গুরুত্বপূর্ণ?**\n",
    "\n",
    "1.  **সম্পূর্ণ চিত্র (Holistic View):** বিভিন্ন উৎস থেকে ডেটা একত্রিত করে একটি সম্পূর্ণ এবং বিস্তারিত চিত্র পাওয়া যায়, যা শুধুমাত্র একটি উৎস থেকে পাওয়া ডেটা দিয়ে সম্ভব নয়। উদাহরণস্বরূপ, একজন গ্রাহকের লেনদেনের ডেটা একটি ডেটাবেসে এবং তার সোশ্যাল মিডিয়া অ্যাক্টিভিটি অন্য একটি সার্ভিসে থাকতে পারে। ডেটা ইন্টিগ্রেশন এই সব ডেটা একসাথে এনে গ্রাহকের একটি সম্পূর্ণ প্রোফাইল তৈরি করে।\n",
    "2.  **উন্নত বিশ্লেষণ (Improved Analysis):** ডেটা একত্রিত হওয়ার ফলে আরও জটিল এবং ব্যাপক বিশ্লেষণ চালানো যায়, যা গভীর ইনসাইট (insights) প্রদান করে।\n",
    "3.  **সমন্বিত সিদ্ধান্ত গ্রহণ (Coordinated Decision Making):** বিভিন্ন বিভাগের ডেটা একত্রিত হলে সমগ্র প্রতিষ্ঠান জুড়ে আরও অবগত এবং সমন্বিত সিদ্ধান্ত গ্রহণ করা সম্ভব হয়।\n",
    "4.  **রিডানডেন্সি হ্রাস (Reduced Redundancy):** ইন্টিগ্রেশন প্রক্রিয়ায় প্রায়শই ডুপ্লিকেট ডেটা শনাক্ত করে তা অপসারণ করা হয়, যা ডেটার রিডানডেন্সি কমায় এবং ডেটা স্টোরেজ ও প্রক্রিয়াকরণের দক্ষতা বাড়ায়।\n",
    "5.  **ডেটা গুণমান বৃদ্ধি (Improved Data Quality):** ডেটা ইন্টিগ্রেশনের সময় ডেটার অসঙ্গতি (inconsistencies) এবং ত্রুটি (errors) সনাক্ত ও সমাধান করা হয়, যা সামগ্রিক ডেটা গুণমান উন্নত করে।\n",
    "\n",
    "---\n",
    "\n",
    "### ডেটা ইন্টিগ্রেশনের প্রধান চ্যালেঞ্জসমূহ (Major Challenges in Data Integration)\n",
    "\n",
    "ডেটা ইন্টিগ্রেশনের সময় বেশ কিছু চ্যালেঞ্জের সম্মুখীন হতে হয়:\n",
    "\n",
    "1.  **এন্ট্রি কনফ্লিক্টস এবং ডেটা রিডানডেন্সি (Entity Conflicts and Data Redundancy):**\n",
    "    * বিভিন্ন ডেটা উৎসে একই এন্ট্রির জন্য ভিন্ন ভিন্ন অ্যাট্রিবিউট নেমিং (যেমন, 'গ্রাহক আইডি' বনাম 'কাস্টমার আইডি') বা ভিন্ন ভিন্ন ডেটা ফরম্যাট থাকতে পারে।\n",
    "    * একই ডেটা একাধিক উৎসে থাকতে পারে (ডুপ্লিকেট ডেটা), যা অসঙ্গতি তৈরি করতে পারে।\n",
    "\n",
    "2.  **স্কিমা ইন্টিগ্রেশন (Schema Integration):**\n",
    "    * বিভিন্ন ডেটা উৎসের ডেটাবেস স্কিমা (table structures, column names, data types) ভিন্ন হতে পারে। এগুলোকে একটি সুসংগত গ্লোবাল স্কিমায় একত্রিত করা কঠিন হতে পারে।\n",
    "    * **সমস্যার উদাহরণ:**\n",
    "        * **এন্ট্রি আইডেন্টিফিকেশন (Entity Identification):** একই বাস্তব-বিশ্বের সত্তার (যেমন, একজন গ্রাহক) জন্য বিভিন্ন উৎসে ভিন্ন ভিন্ন আইডি থাকতে পারে।\n",
    "        * **ডেটা ভ্যালু কনফ্লিক্টস (Data Value Conflicts):** একই অ্যাট্রিবিউটের জন্য বিভিন্ন উৎসে ভিন্ন মান থাকতে পারে (যেমন, একজন গ্রাহকের বয়স একটি উৎসে 30 এবং অন্যটিতে 31)।\n",
    "        * **ডেটা টাইপ কনফ্লিক্টস (Data Type Conflicts):** একই অ্যাট্রিবিউটের জন্য বিভিন্ন উৎসে ভিন্ন ডেটা টাইপ থাকতে পারে (যেমন, ফোন নম্বর একটিতে সংখ্যা এবং অন্যটিতে টেক্সট)।\n",
    "\n",
    "3.  **কনসিস্টেন্সি ইস্যু (Consistency Issues):**\n",
    "    * ডেটা একত্রিত করার সময় ডেটার মান, ডেটা টাইপ এবং ফরম্যাটের মধ্যে অসঙ্গতি দেখা দিতে পারে।\n",
    "\n",
    "---\n",
    "\n",
    "### ডেটা ইন্টিগ্রেশনের প্রধান কৌশলসমূহ (Key Techniques in Data Integration)\n",
    "\n",
    "ডেটা ইন্টিগ্রেশন সাধারণত ETL (Extract, Transform, Load) প্রক্রিয়ার মাধ্যমে সম্পন্ন হয়। তবে, মেশিন লার্নিংয়ের ডেটা প্রিপ্রসেসিংয়ের প্রসঙ্গে আমরা প্রধানত ডেটা একত্রিত করার দিকটি দেখবো।\n",
    "\n",
    "1.  **ডেটা একত্রিত করা (Combining Data):**\n",
    "\n",
    "    a.  **মার্জ (Merge) / জয়েন (Join):**\n",
    "        * **কখন ব্যবহার করবেন:** যখন আপনার কাছে একাধিক ডেটাফ্রেম বা টেবিল থাকে এবং আপনি একটি বা একাধিক সাধারণ কলামের (key) উপর ভিত্তি করে সেগুলিকে একত্রিত করতে চান। এটি ডেটাবেস টেবিল জয়েন করার মতোই।\n",
    "        * **উদাহরণ:** গ্রাহকের ব্যক্তিগত তথ্য (গ্রাহক আইডি, নাম) একটি টেবিল এবং তার কেনাকাটার তথ্য (গ্রাহক আইডি, পণ্যের নাম, দাম) অন্য একটি টেবিলে থাকলে, 'গ্রাহক আইডি' ব্যবহার করে দুটি টেবিলকে মার্জ করা যায়।\n",
    "        * **প্রকারভেদ:** Inner Join, Left Join, Right Join, Outer Join (Pandas এ `pd.merge()` ব্যবহার করা হয়)।\n",
    "\n",
    "        ```python\n",
    "        # Python উদাহরণ (Pandas ব্যবহার করে)\n",
    "        import pandas as pd\n",
    "\n",
    "        # ডেটাফ্রেম 1\n",
    "        df1 = pd.DataFrame({'customer_id': [1, 2, 3, 4],\n",
    "                            'name': ['Alice', 'Bob', 'Charlie', 'David']})\n",
    "\n",
    "        # ডেটাফ্রেম 2\n",
    "        df2 = pd.DataFrame({'customer_id': [1, 3, 5, 2],\n",
    "                            'order_id': ['A1', 'C1', 'E1', 'B1'],\n",
    "                            'amount': [100, 200, 300, 150]})\n",
    "\n",
    "        print(\"DataFrame 1 (Customer Info):\\n\", df1)\n",
    "        print(\"\\nDataFrame 2 (Order Info):\\n\", df2)\n",
    "\n",
    "        # customer_id কলামের উপর ভিত্তি করে Inner Join\n",
    "        # শুধুমাত্র যে customer_id উভয় DataFrame-এ আছে, সেগুলো দেখাবে।\n",
    "        merged_df_inner = pd.merge(df1, df2, on='customer_id', how='inner')\n",
    "        print(\"\\nMerged DataFrame (Inner Join):\\n\", merged_df_inner)\n",
    "\n",
    "        # Left Join (df1 এর সব ডেটা থাকবে, df2 থেকে matching data আসবে)\n",
    "        merged_df_left = pd.merge(df1, df2, on='customer_id', how='left')\n",
    "        print(\"\\nMerged DataFrame (Left Join):\\n\", merged_df_left)\n",
    "        ```\n",
    "\n",
    "    b.  **কনক্যাটেনেশন (Concatenation):**\n",
    "        * **কখন ব্যবহার করবেন:** যখন আপনার কাছে একই স্ট্রাকচার (একই কলাম হেডার) সহ একাধিক ডেটাফ্রেম থাকে এবং আপনি সেগুলিকে সারি (rows) বরাবর বা কলাম (columns) বরাবর একত্রিত করতে চান।\n",
    "        * **উদাহরণ:** বিভিন্ন মাসের বিক্রয় ডেটা (যেখানে কলামগুলি একই) একত্রিত করা।\n",
    "        * `pd.concat()` ব্যবহার করা হয়।\n",
    "\n",
    "        ```python\n",
    "        # কনক্যাটেনেশন উদাহরণ\n",
    "        df_jan = pd.DataFrame({'product': ['A', 'B'], 'sales': [100, 150]})\n",
    "        df_feb = pd.DataFrame({'product': ['C', 'A'], 'sales': [200, 120]})\n",
    "\n",
    "        print(\"\\nSales Jan:\\n\", df_jan)\n",
    "        print(\"\\nSales Feb:\\n\", df_feb)\n",
    "\n",
    "        # সারি বরাবর কনক্যাটেনেশন\n",
    "        all_sales = pd.concat([df_jan, df_feb], axis=0) # axis=0 rows এর জন্য\n",
    "        print(\"\\nAll Sales (Concatenated by rows):\\n\", all_sales)\n",
    "\n",
    "        # কলাম বরাবর কনক্যাটেনেশন (যদি কলামগুলি একই অর্থ বহন করে এবং প্রতিটি সারিতে matching data থাকে)\n",
    "        # সাধারণত এটি Join এর মতো কাজ করে না যদি index match না করে।\n",
    "        # এখানে উদাহরণ হিসেবে দেখানোর জন্য এমন কলাম নেই যা পাশাপাশি যুক্ত করা যায়।\n",
    "        # কিন্তু যদি আপনি ডেটাফ্রেমগুলোকে পাশাপাশি যুক্ত করতে চান:\n",
    "        # pd.concat([df_a, df_b], axis=1)\n",
    "        ```\n",
    "\n",
    "2.  **স্কিমা ইন্টিগ্রেশন (Schema Integration):**\n",
    "    * এটি বিভিন্ন উৎসের ডেটাবেস স্কিমা বা ডেটাফ্রেমের স্ট্রাকচারকে একটি সামঞ্জস্যপূর্ণ ফরম্যাটে নিয়ে আসার প্রক্রিয়া।\n",
    "    * **নামের অসঙ্গতি ঠিক করা (Handling Naming Conflicts):**\n",
    "        * বিভিন্ন উৎসে একই তথ্যের জন্য ভিন্ন ভিন্ন অ্যাট্রিবিউট নাম থাকতে পারে (যেমন `Cust_ID`, `Customer ID`, `CustomerID`)। এই নামগুলোকে একটি সাধারণ এবং মানসম্মত নামে পরিবর্তন করা হয়।\n",
    "        * **উপায়:** কলাম রিনেম (rename) করা।\n",
    "\n",
    "        ```python\n",
    "        # কলাম রিনেম উদাহরণ\n",
    "        df_inconsistent = pd.DataFrame({'Cust_ID': [1, 2], 'CustName': ['A', 'B']})\n",
    "        print(\"\\nInconsistent DataFrame:\\n\", df_inconsistent)\n",
    "\n",
    "        df_consistent = df_inconsistent.rename(columns={'Cust_ID': 'CustomerID', 'CustName': 'CustomerName'})\n",
    "        print(\"\\nConsistent DataFrame (after renaming columns):\\n\", df_consistent)\n",
    "        ```\n",
    "\n",
    "    * **ডেটা টাইপ সামঞ্জস্যপূর্ণ করা (Ensuring Data Type Consistency):**\n",
    "        * বিভিন্ন উৎসে একই অ্যাট্রিবিউটের জন্য ভিন্ন ডেটা টাইপ থাকতে পারে (যেমন, 'বয়স' একটিতে সংখ্যা এবং অন্যটিতে টেক্সট)। ডেটা টাইপগুলিকে একটি সাধারণ টাইপে রূপান্তর করা হয়।\n",
    "        * **উপায়:** ডেটা টাইপ কাস্ট (cast) করা।\n",
    "\n",
    "        ```python\n",
    "        # ডেটা টাইপ কনভার্সন উদাহরণ\n",
    "        df_dtype = pd.DataFrame({'ID': ['1', '2', '3'], 'Value': ['10', '20', '30']})\n",
    "        print(\"\\nOriginal DataFrame (with inconsistent data types):\\n\", df_dtype)\n",
    "        print(df_dtype.dtypes)\n",
    "\n",
    "        df_dtype['ID'] = df_dtype['ID'].astype(int)\n",
    "        df_dtype['Value'] = df_dtype['Value'].astype(float)\n",
    "        print(\"\\nDataFrame (after converting data types):\\n\", df_dtype)\n",
    "        print(df_dtype.dtypes)\n",
    "        ```\n",
    "\n",
    "3.  **ডেটা ভ্যালু কনফ্লিক্টস হ্যান্ডলিং (Handling Data Value Conflicts):**\n",
    "    * যদি একই অ্যাট্রিবিউটের জন্য বিভিন্ন উৎসে ভিন্ন মান থাকে (যেমন, একজন ব্যক্তির ঠিকানা দুটি ভিন্ন সিস্টেমে সামান্য ভিন্ন), তবে কোনটি সঠিক বা সবচেয়ে আপডেটেড তা সিদ্ধান্ত নিতে হয়।\n",
    "    * **উপায়:**\n",
    "        * **ডেটা প্রায়োরিটি (Data Priority):** একটি নির্দিষ্ট উৎসের ডেটাকে অন্য উৎসের চেয়ে বেশি অগ্রাধিকার দেওয়া।\n",
    "        * **সর্বশেষ আপডেট (Latest Update):** সবচেয়ে সাম্প্রতিক আপডেট হওয়া ডেটাকে বেছে নেওয়া।\n",
    "        * **ভোট/অ্যাভারেজ (Voting/Averaging):** একাধিক উৎস থেকে পাওয়া মানগুলির গড় নেওয়া।\n",
    "        * **ম্যানুয়াল ইন্টারভেনশন (Manual Intervention):** কিছু ক্ষেত্রে ম্যানুয়ালি যাচাই করা প্রয়োজন হতে পারে।\n",
    "\n",
    "4.  **ডেটা রিডানডেন্সি এবং ইনকনসিস্টেন্সি হ্যান্ডলিং (Handling Data Redundancy and Inconsistency):**\n",
    "    * ডেটা ইন্টিগ্রেশনের সময় প্রায়শই ডুপ্লিকেট ডেটা বা অসঙ্গতিপূর্ণ ডেটা তৈরি হয়।\n",
    "    * **ডুপ্লিকেট রিমুভাল:** আগেই আলোচনা করা হয়েছে (ডেটা ক্লিনিং এর অংশ)।\n",
    "    * **ইনকনসিস্টেন্সি রেজোলিউশন:** ডেটা ক্লিনিং এবং ইন্টিগ্রেশন রুলস ব্যবহার করে অসঙ্গতিপূর্ণ ডেটা ঠিক করা।\n",
    "\n",
    "---\n",
    "\n",
    "**ডেটা ইন্টিগ্রেশনের কার্যকরী অনুশীলন (Best Practices for Data Integration):**\n",
    "\n",
    "* **ডেটা প্রোফাইলিং (Data Profiling):** ইন্টিগ্রেশনের আগে প্রতিটি ডেটা উৎসের ডেটা স্ট্রাকচার, গুণমান এবং বিষয়বস্তু পুঙ্খানুপুঙ্খভাবে বিশ্লেষণ করা।\n",
    "* **মেটাডেটা ম্যানেজমেন্ট (Metadata Management):** ডেটা উৎস, স্কিমা এবং ইন্টিগ্রেশন নিয়মাবলী সম্পর্কে বিস্তারিত মেটাডেটা বজায় রাখা।\n",
    "* **ডেটা ট্রান্সফরমেশন টুলস (Data Transformation Tools):** ETL টুলস বা প্রোগ্রামিং লাইব্রেরি (যেমন Pandas, PySpark) ব্যবহার করে ডেটা ট্রান্সফরমেশন এবং ইন্টিগ্রেশন প্রক্রিয়াকে স্বয়ংক্রিয় করা।\n",
    "* **ইন্টিগ্রেশন স্ট্র্যাটেজি (Integration Strategy):** আপনার প্রয়োজনের উপর ভিত্তি করে সঠিক ইন্টিগ্রেশন স্ট্র্যাটেজি (যেমন ডেটা ওয়্যারহাউস, ডেটা লেক, ফেডারেটেড ডেটাবেস) নির্বাচন করা।\n",
    "\n",
    "ডেটা ইন্টিগ্রেশন মেশিন লার্নিং মডেলের জন্য একটি সম্পূর্ণ এবং নির্ভরযোগ্য ডেটাসেট নিশ্চিত করতে সাহায্য করে। এটি জটিল কিন্তু একটি অপরিহার্য প্রক্রিয়া।\n",
    "\n",
    "আপনার কি ডেটা ইন্টিগ্রেশনের কোনো নির্দিষ্ট অংশ নিয়ে আরও বিস্তারিত জানার প্রয়োজন আছে, অথবা কোনো প্রশ্ন আছে? এরপর আমরা **ডেটা ট্রান্সফরমেশন (Data Transformation)** নিয়ে আলোচনা করব।"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2885ef2",
   "metadata": {},
   "source": [
    "আচ্ছা, প্রতিটি ডেটা প্রিপ্রসেসিং ধাপের জন্য আমি আলাদা আলাদা কোড উদাহরণ দেবো। প্রতিটি উদাহরণের শুরুতে একটি সংক্ষিপ্ত বিবরণ থাকবে এবং কোডগুলো প্রধানত Pandas এবং Scikit-learn লাইব্রেরি ব্যবহার করে লেখা হবে, যা পাইথনে ডেটা সায়েন্সের জন্য সবচেয়ে বেশি ব্যবহৃত হয়।\n",
    "\n",
    "প্রতিটি ধাপের জন্য একটি করে কোড ব্লক থাকবে। আপনি চাইলে একটি Jupyter Notebook বা Python স্ক্রিপ্টে এই কোডগুলি ধাপে ধাপে চালাতে পারেন।\n",
    "\n",
    "1. ডেটা পরিষ্কার করা (Data Cleaning) - কোড উদাহরণ\n",
    "1.1. মিসিং ভ্যালু হ্যান্ডলিং (Handling Missing Values)\n",
    "ডেটা সেট তৈরি:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# একটি উদাহরণ ডেটাফ্রেম তৈরি করি যেখানে মিসিং ভ্যালু আছে\n",
    "data = {'Feature1': [1, 2, np.nan, 4, 5],\n",
    "        'Feature2': [6, np.nan, 8, 9, np.nan],\n",
    "        'Feature3': [11, 12, 13, 14, 15],\n",
    "        'Category': ['A', 'B', 'A', np.nan, 'C']}\n",
    "df_missing = pd.DataFrame(data)\n",
    "print(\"Original DataFrame with Missing Values:\\n\", df_missing)\n",
    "print(\"\\nMissing values count per column:\\n\", df_missing.isnull().sum())\n",
    "কৌশল 1: মিসিং ভ্যালু সহ রো বাদ দেওয়া (Dropping Rows with Missing Values)\n",
    "\n",
    "Python\n",
    "\n",
    "# মিসিং ভ্যালু সহ রো বাদ দেওয়া\n",
    "df_dropped_rows = df_missing.dropna()\n",
    "print(\"\\nDataFrame after dropping rows with any missing values:\\n\", df_dropped_rows)\n",
    "\n",
    "# শুধুমাত্র নির্দিষ্ট কলামে (যেমন 'Feature1') মিসিং ভ্যালু থাকলে সেই রো বাদ দেওয়া\n",
    "df_dropped_specific_col = df_missing.dropna(subset=['Feature1'])\n",
    "print(\"\\nDataFrame after dropping rows with missing values in 'Feature1':\\n\", df_dropped_specific_col)\n",
    "কৌশল 2: মিসিং ভ্যালু ইমপিউট করা (Imputing Missing Values)\n",
    "\n",
    "কৌশল 2.1: মিন (Mean) / মিডিয়ান (Median) / মোড (Mode) দিয়ে পূরণ\n",
    "\n",
    "Python\n",
    "\n",
    "# মিন দিয়ে সংখ্যাগত কলাম পূরণ\n",
    "df_mean_imputed = df_missing.copy()\n",
    "df_mean_imputed['Feature1'].fillna(df_mean_imputed['Feature1'].mean(), inplace=True)\n",
    "df_mean_imputed['Feature2'].fillna(df_mean_imputed['Feature2'].mean(), inplace=True)\n",
    "print(\"\\nDataFrame after Mean Imputation for numerical columns:\\n\", df_mean_imputed)\n",
    "\n",
    "# মিডিয়ান দিয়ে সংখ্যাগত কলাম পূরণ\n",
    "df_median_imputed = df_missing.copy()\n",
    "df_median_imputed['Feature1'].fillna(df_median_imputed['Feature1'].median(), inplace=True)\n",
    "df_median_imputed['Feature2'].fillna(df_median_imputed['Feature2'].median(), inplace=True)\n",
    "print(\"\\nDataFrame after Median Imputation for numerical columns:\\n\", df_median_imputed)\n",
    "\n",
    "# মোড দিয়ে ক্যাটাগরিক্যাল কলাম পূরণ\n",
    "df_mode_imputed = df_missing.copy()\n",
    "# mode() একটি সিরিজ রিটার্ন করে, তাই [0] দিয়ে প্রথম মোডটি নিতে হয়\n",
    "df_mode_imputed['Category'].fillna(df_mode_imputed['Category'].mode()[0], inplace=True)\n",
    "print(\"\\nDataFrame after Mode Imputation for 'Category' column:\\n\", df_mode_imputed)\n",
    "কৌশল 2.2: ফরোয়ার্ড ফিল (Forward Fill) / ব্যাকওয়ার্ড ফিল (Backward Fill)\n",
    "\n",
    "Python\n",
    "\n",
    "# ফরোয়ার্ড ফিল (ffill)\n",
    "df_ffill = df_missing.copy()\n",
    "df_ffill.fillna(method='ffill', inplace=True)\n",
    "print(\"\\nDataFrame after Forward Fill:\\n\", df_ffill)\n",
    "\n",
    "# ব্যাকওয়ার্ড ফিল (bfill)\n",
    "df_bfill = df_missing.copy()\n",
    "df_bfill.fillna(method='bfill', inplace=True)\n",
    "print(\"\\nDataFrame after Backward Fill:\\n\", df_bfill)\n",
    "কৌশল 2.3: KNN Imputer ব্যবহার করে পূরণ\n",
    "\n",
    "Python\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# শুধুমাত্র নিউমেরিক ডেটা নিয়ে কাজ করতে হবে\n",
    "df_knn_data = df_missing[['Feature1', 'Feature2', 'Feature3']].copy()\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2) # 2টি নিকটতম প্রতিবেশী ব্যবহার করে\n",
    "df_knn_imputed_array = imputer.fit_transform(df_knn_data)\n",
    "df_knn_imputed = pd.DataFrame(df_knn_imputed_array, columns=df_knn_data.columns)\n",
    "\n",
    "# আসল ডেটাফ্রেমে ফিরে যোগ করা (ক্যাটাগরিক্যাল কলাম সহ)\n",
    "df_final_knn_imputed = df_missing.copy()\n",
    "df_final_knn_imputed['Feature1'] = df_knn_imputed['Feature1']\n",
    "df_final_knn_imputed['Feature2'] = df_knn_imputed['Feature2']\n",
    "print(\"\\nDataFrame after KNN Imputation (numerical columns):\\n\", df_final_knn_imputed)\n",
    "1.2. নয়েজি ডেটা হ্যান্ডলিং (Handling Noisy Data) - বিন্নিং\n",
    "ডেটা সেট তৈরি:\n",
    "\n",
    "Python\n",
    "\n",
    "# একটি উদাহরণ ডেটাফ্রেম তৈরি করি যেখানে নয়েজ থাকতে পারে (যেমন 150 বয়স)\n",
    "data_noisy_binning = {'Age': [20, 25, 28, 32, 35, 40, 43, 48, 50, 150]} # 150 একটি নয়েজ\n",
    "df_noisy_binning = pd.DataFrame(data_noisy_binning)\n",
    "print(\"\\nOriginal DataFrame for Binning (with potential noise):\\n\", df_noisy_binning)\n",
    "কৌশল: বিন্নিং (Binning)\n",
    "\n",
    "Python\n",
    "\n",
    "# সমান প্রস্থের বিন্নিং (4টি বিন)\n",
    "# bins=4 স্বয়ংক্রিয়ভাবে 4টি সমান প্রস্থের বিন তৈরি করবে।\n",
    "# labels=False দিলে 0, 1, 2, 3 হিসাবে বিনের ইন্ডেক্স দেবে।\n",
    "df_noisy_binning['Age_Bin_Equal_Width'] = pd.cut(df_noisy_binning['Age'], bins=4, labels=False)\n",
    "print(\"\\nDataFrame after Equal-Width Binning:\\n\", df_noisy_binning)\n",
    "\n",
    "# কাস্টম বিন্স দিয়ে বিন্নিং (নয়েজ হ্যান্ডেল করার জন্য)\n",
    "# এখানে 150 কে 'Outlier' বিনে ফেলা হচ্ছে\n",
    "custom_bins = [0, 30, 55, 200]\n",
    "custom_labels = ['Young', 'Adult', 'Outlier']\n",
    "df_noisy_binning['Age_Custom_Bin'] = pd.cut(df_noisy_binning['Age'], bins=custom_bins, labels=custom_labels, right=False)\n",
    "print(\"\\nDataFrame after Custom Binning:\\n\", df_noisy_binning)\n",
    "1.3. আউটলায়ার হ্যান্ডলিং (Handling Outliers)\n",
    "ডেটা সেট তৈরি:\n",
    "\n",
    "Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# একটি উদাহরণ ডেটাফ্রেম তৈরি করি যেখানে আউটলায়ার আছে\n",
    "data_outlier = {'Value': [10, 12, 15, 13, 16, 18, 20, 22, 25, 100, 28, 30, 5]} # 100 একটি আউটলায়ার\n",
    "df_outlier = pd.DataFrame(data_outlier)\n",
    "print(\"\\nOriginal DataFrame for Outlier Detection:\\n\", df_outlier)\n",
    "কৌশল 1: বক্স প্লট (Box Plot) দ্বারা সনাক্তকরণ এবং IQR ব্যবহার করে বাদ দেওয়া\n",
    "\n",
    "Python\n",
    "\n",
    "# বক্স প্লট দিয়ে ভিজ্যুয়ালি আউটলায়ার সনাক্তকরণ\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(y=df_outlier['Value'])\n",
    "plt.title(\"Box Plot to Detect Outliers\")\n",
    "plt.show()\n",
    "\n",
    "# IQR ব্যবহার করে আউটলায়ার চিহ্নিত করা এবং বাদ দেওয়া\n",
    "Q1 = df_outlier['Value'].quantile(0.25)\n",
    "Q3 = df_outlier['Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_iqr = df_outlier[(df_outlier['Value'] < lower_bound) | (df_outlier['Value'] > upper_bound)]\n",
    "print(f\"\\nOutliers detected using IQR: {outliers_iqr}\")\n",
    "\n",
    "df_no_outliers_iqr = df_outlier[(df_outlier['Value'] >= lower_bound) & (df_outlier['Value'] <= upper_bound)]\n",
    "print(f\"\\nDataFrame after removing outliers (using IQR):\\n\", df_no_outliers_iqr)\n",
    "কৌশল 2: Z-Score ব্যবহার করে সনাক্তকরণ এবং বাদ দেওয়া\n",
    "\n",
    "Python\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Z-Score গণনা\n",
    "df_outlier['Z_Score'] = np.abs(stats.zscore(df_outlier['Value']))\n",
    "\n",
    "# Z-Score থ্রেশহোল্ড (যেমন 2 বা 3) ব্যবহার করে আউটলায়ার চিহ্নিত করা\n",
    "# এখানে 2 ব্যবহার করছি, যা সাধারণত 95% ডেটা কভার করে\n",
    "outliers_zscore = df_outlier[df_outlier['Z_Score'] > 2]\n",
    "print(f\"\\nOutliers detected using Z-Score (threshold > 2): {outliers_zscore}\")\n",
    "\n",
    "# আউটলায়ার বাদ দেওয়া\n",
    "df_no_outliers_zscore = df_outlier[df_outlier['Z_Score'] <= 2].drop(columns=['Z_Score'])\n",
    "print(f\"\\nDataFrame after removing outliers (using Z-Score):\\n\", df_no_outliers_zscore)\n",
    "কৌশল 3: আউটলায়ার ক্যাপিং (Capping Outliers)\n",
    "\n",
    "Python\n",
    "\n",
    "# 95তম এবং 5তম পার্সেন্টাইল ব্যবহার করে ক্যাপিং\n",
    "df_capped = df_outlier.copy()\n",
    "upper_cap = df_capped['Value'].quantile(0.95)\n",
    "lower_cap = df_capped['Value'].quantile(0.05)\n",
    "\n",
    "# উপরের বাউন্ডের চেয়ে বড় মানগুলিকে upper_cap দিয়ে প্রতিস্থাপন করা\n",
    "df_capped['Value'] = np.where(df_capped['Value'] > upper_cap, upper_cap, df_capped['Value'])\n",
    "# নিচের বাউন্ডের চেয়ে ছোট মানগুলিকে lower_cap দিয়ে প্রতিস্থাপন করা\n",
    "df_capped['Value'] = np.where(df_capped['Value'] < lower_cap, lower_cap, df_capped['Value'])\n",
    "\n",
    "print(f\"\\nDataFrame after capping outliers (5th and 95th percentiles):\\n\", df_capped[['Value']])\n",
    "1.4. ডুপ্লিকেট ভ্যালু রিমুভাল (Duplicate Value Removal)\n",
    "ডেটা সেট তৈরি:\n",
    "\n",
    "Python\n",
    "\n",
    "# একটি উদাহরণ ডেটাফ্রেম তৈরি করি যেখানে ডুপ্লিকেট ভ্যালু আছে\n",
    "data_duplicates = {'ID': [1, 2, 3, 1, 4, 2, 5],\n",
    "                   'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'David', 'Bob', 'Eve'],\n",
    "                   'Age': [25, 30, 35, 25, 40, 30, 45]}\n",
    "df_duplicates = pd.DataFrame(data_duplicates)\n",
    "print(\"Original DataFrame with Duplicates:\\n\", df_duplicates)\n",
    "কৌশল: ডুপ্লিকেট রো বাদ দেওয়া\n",
    "\n",
    "Python\n",
    "\n",
    "# সমস্ত কলামের উপর ভিত্তি করে ডুপ্লিকেট রো সনাক্ত করা\n",
    "print(\"\\nDuplicate rows (based on all columns):\\n\", df_duplicates[df_duplicates.duplicated()])\n",
    "\n",
    "# ডুপ্লিকেট রো বাদ দেওয়া (প্রথমটি রেখে বাকি বাদ)\n",
    "df_no_duplicates_all = df_duplicates.drop_duplicates()\n",
    "print(\"\\nDataFrame after dropping duplicates (all columns):\\n\", df_no_duplicates_all)\n",
    "\n",
    "# নির্দিষ্ট কলামের উপর ভিত্তি করে ডুপ্লিকেট বাদ দেওয়া (যেমন 'ID' এবং 'Name' কলাম)\n",
    "df_no_duplicates_subset = df_duplicates.drop_duplicates(subset=['ID', 'Name'])\n",
    "print(\"\\nDataFrame after dropping duplicates based on 'ID' and 'Name' columns:\\n\", df_no_duplicates_subset)\n",
    "2. ডেটা ইন্টিগ্রেশন (Data Integration) - কোড উদাহরণ\n",
    "ডেটা সেট তৈরি:\n",
    "\n",
    "Python\n",
    "\n",
    "# দুটি উদাহরণ ডেটাফ্রেম তৈরি করি\n",
    "df_customers = pd.DataFrame({\n",
    "    'customer_id': [101, 102, 103, 104, 105],\n",
    "    'customer_name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'city': ['NYC', 'LA', 'Chicago', 'NYC', 'LA']\n",
    "})\n",
    "\n",
    "df_orders = pd.DataFrame({\n",
    "    'order_id': ['O1', 'O2', 'O3', 'O4', 'O5'],\n",
    "    'customer_id': [101, 103, 106, 102, 101], # 106 এখানে df_customers এ নেই\n",
    "    'product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Laptop'],\n",
    "    'amount': [1200, 25, 75, 300, 1200]\n",
    "})\n",
    "\n",
    "df_sales_q1 = pd.DataFrame({\n",
    "    'date': ['2024-01-10', '2024-02-15'],\n",
    "    'product': ['Laptop', 'Mouse'],\n",
    "    'quantity': [5, 10]\n",
    "})\n",
    "\n",
    "df_sales_q2 = pd.DataFrame({\n",
    "    'date': ['2024-04-05', '2024-05-20'],\n",
    "    'product': ['Monitor', 'Keyboard'],\n",
    "    'quantity': [8, 12]\n",
    "})\n",
    "\n",
    "print(\"DataFrame: Customers\\n\", df_customers)\n",
    "print(\"\\nDataFrame: Orders\\n\", df_orders)\n",
    "print(\"\\nDataFrame: Sales Q1\\n\", df_sales_q1)\n",
    "print(\"\\nDataFrame: Sales Q2\\n\", df_sales_q2)\n",
    "কৌশল 1: মার্জ (Merge) / জয়েন (Join)\n",
    "\n",
    "Python\n",
    "\n",
    "# Inner Join: শুধুমাত্র যে customer_id উভয় DataFrame-এ আছে, সেগুলো দেখাবে।\n",
    "merged_inner_df = pd.merge(df_customers, df_orders, on='customer_id', how='inner')\n",
    "print(\"\\nMerged DataFrame (Inner Join on customer_id):\\n\", merged_inner_df)\n",
    "\n",
    "# Left Join: df_customers এর সব ডেটা থাকবে, df_orders থেকে matching data আসবে।\n",
    "merged_left_df = pd.merge(df_customers, df_orders, on='customer_id', how='left')\n",
    "print(\"\\nMerged DataFrame (Left Join on customer_id):\\n\", merged_left_df)\n",
    "\n",
    "# Outer Join: উভয় DataFrame থেকে সমস্ত ডেটা থাকবে, non-matching এর জন্য NaN থাকবে।\n",
    "merged_outer_df = pd.merge(df_customers, df_orders, on='customer_id', how='outer')\n",
    "print(\"\\nMerged DataFrame (Outer Join on customer_id):\\n\", merged_outer_df)\n",
    "কৌশল 2: কনক্যাটেনেশন (Concatenation)\n",
    "\n",
    "Python\n",
    "\n",
    "# সারি বরাবর কনক্যাটেনেশন (যখন কলাম একই থাকে)\n",
    "all_sales_df = pd.concat([df_sales_q1, df_sales_q2], axis=0) # axis=0 rows এর জন্য\n",
    "print(\"\\nConcatenated DataFrame (All Sales):\\n\", all_sales_df)\n",
    "কৌশল 3: স্কিমা ইন্টিগ্রেশন - কলাম রিনেম এবং ডেটা টাইপ কনভার্সন\n",
    "\n",
    "Python\n",
    "\n",
    "# কলাম রিনেম উদাহরণ (যদি একটি ডেটাফ্রেমে কলামের নাম ভিন্ন হয়)\n",
    "df_customers_alt = pd.DataFrame({\n",
    "    'cust_id': [101, 102],\n",
    "    'cust_name': ['Anna', 'Ben']\n",
    "})\n",
    "print(\"\\nOriginal DataFrame (with inconsistent column names):\\n\", df_customers_alt)\n",
    "\n",
    "df_customers_renamed = df_customers_alt.rename(columns={'cust_id': 'customer_id', 'cust_name': 'customer_name'})\n",
    "print(\"\\nDataFrame after renaming columns:\\n\", df_customers_renamed)\n",
    "\n",
    "# ডেটা টাইপ কনভার্সন উদাহরণ\n",
    "df_dtype_inconsistent = pd.DataFrame({'ID': ['1', '2'], 'Amount_Str': ['10.5', '20.0']})\n",
    "print(\"\\nDataFrame with inconsistent data types:\\n\", df_dtype_inconsistent)\n",
    "print(\"Dtypes before conversion:\\n\", df_dtype_inconsistent.dtypes)\n",
    "\n",
    "df_dtype_consistent = df_dtype_inconsistent.copy()\n",
    "df_dtype_consistent['ID'] = df_dtype_consistent['ID'].astype(int)\n",
    "df_dtype_consistent['Amount_Str'] = df_dtype_consistent['Amount_Str'].astype(float)\n",
    "print(\"\\nDataFrame after converting data types:\\n\", df_dtype_consistent)\n",
    "print(\"Dtypes after conversion:\\n\", df_dtype_consistent.dtypes)\n",
    "3. ডেটা ট্রান্সফরমেশন (Data Transformation) - কোড উদাহরণ\n",
    "ডেটা সেট তৈরি:\n",
    "\n",
    "Python\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # ভিজ্যুয়ালাইজেশনের জন্য\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # হিস্টোগ্রাম ওয়ার্নিং লুকানোর জন্য\n",
    "\n",
    "data_transform = {'FeatureA': [10, 20, 30, 40, 50, 100], # একটি আউটলায়ার সহ\n",
    "                  'FeatureB': [1000, 2000, 3000, 4000, 5000, 6000],\n",
    "                  'Age': [18, 22, 25, 30, 35, 40, 45, 50, 55, 60]}\n",
    "df_transform = pd.DataFrame(data_transform)\n",
    "print(\"Original DataFrame for Transformations:\\n\", df_transform)\n",
    "3.1. নরম্যালাইজেশন (Normalization) এবং স্কেলিং (Scaling)\n",
    "Min-Max Normalization (0-1 এর মধ্যে স্কেল করা)\n",
    "\n",
    "Python\n",
    "\n",
    "# Min-Max Normalization\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_minmax_scaled = pd.DataFrame(scaler_minmax.fit_transform(df_transform), columns=df_transform.columns)\n",
    "print(\"\\nDataFrame after Min-Max Normalization:\\n\", df_minmax_scaled)\n",
    "\n",
    "# ভিজ্যুয়ালাইজেশন (শুধুমাত্র FeatureA এর জন্য)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_transform['FeatureA'], kde=True)\n",
    "plt.title(\"Original FeatureA Distribution\")\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_minmax_scaled['FeatureA'], kde=True)\n",
    "plt.title(\"Min-Max Scaled FeatureA Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "Z-Score Standardization (গড় 0, স্ট্যান্ডার্ড ডেভিয়েশন 1)\n",
    "\n",
    "Python\n",
    "\n",
    "# Z-Score Standardization\n",
    "scaler_standard = StandardScaler()\n",
    "df_standard_scaled = pd.DataFrame(scaler_standard.fit_transform(df_transform), columns=df_transform.columns)\n",
    "print(\"\\nDataFrame after Z-Score Standardization:\\n\", df_standard_scaled)\n",
    "\n",
    "# ভিজ্যুয়ালাইজেশন (শুধুমাত্র FeatureA এর জন্য)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_transform['FeatureA'], kde=True)\n",
    "plt.title(\"Original FeatureA Distribution\")\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_standard_scaled['FeatureA'], kde=True)\n",
    "plt.title(\"Standard Scaled FeatureA Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "Robust Scaler (আউটলায়ার দ্বারা কম প্রভাবিত)\n",
    "\n",
    "Python\n",
    "\n",
    "# Robust Scaler\n",
    "scaler_robust = RobustScaler()\n",
    "df_robust_scaled = pd.DataFrame(scaler_robust.fit_transform(df_transform), columns=df_transform.columns)\n",
    "print(\"\\nDataFrame after Robust Scaling:\\n\", df_robust_scaled)\n",
    "\n",
    "# ভিজ্যুয়ালাইজেশন (শুধুমাত্র FeatureA এর জন্য)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_transform['FeatureA'], kde=True)\n",
    "plt.title(\"Original FeatureA Distribution\")\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_robust_scaled['FeatureA'], kde=True)\n",
    "plt.title(\"Robust Scaled FeatureA Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "MaxAbs Scaler (-1 থেকে 1 এর মধ্যে স্কেল, স্পার্স ডেটার জন্য কার্যকর)\n",
    "\n",
    "Python\n",
    "\n",
    "# MaxAbs Scaler\n",
    "scaler_maxabs = MaxAbsScaler()\n",
    "df_maxabs_scaled = pd.DataFrame(scaler_maxabs.fit_transform(df_transform), columns=df_transform.columns)\n",
    "print(\"\\nDataFrame after MaxAbs Scaling:\\n\", df_maxabs_scaled)\n",
    "3.2. ডেটা ডিসক্রেটাইজেশন / বিন্নিং (Data Discretization / Binning)\n",
    "Python\n",
    "\n",
    "# ডেটাফ্রেম তৈরি\n",
    "df_discretize = pd.DataFrame({'Score': [10, 25, 30, 45, 50, 65, 70, 85, 90, 95]})\n",
    "print(\"\\nOriginal DataFrame for Discretization:\\n\", df_discretize)\n",
    "\n",
    "# সমান প্রস্থের বিন্নিং (Equal-Width Binning) - 4টি বিন\n",
    "# `bins` সংখ্যা নির্দেশ করে বিনের সংখ্যা। `labels=False` বিনের ইনডেক্স দেবে (0, 1, 2, 3)।\n",
    "df_discretize['Score_Bins_Equal_Width'] = pd.cut(df_discretize['Score'], bins=4, labels=False)\n",
    "print(\"\\nDataFrame after Equal-Width Binning:\\n\", df_discretize)\n",
    "\n",
    "# সমান ফ্রিকোয়েন্সির বিন্নিং (Equal-Frequency Binning) - 3টি বিন (quantile binning)\n",
    "# `q` সংখ্যা নির্দেশ করে কোয়ান্টাইলের সংখ্যা।\n",
    "df_discretize['Score_Bins_Equal_Freq'] = pd.qcut(df_discretize['Score'], q=3, labels=False)\n",
    "print(\"\\nDataFrame after Equal-Frequency Binning:\\n\", df_discretize)\n",
    "\n",
    "# কাস্টম বিন্স দিয়ে বিন্নিং\n",
    "custom_bins = [0, 40, 70, 100] # রেঞ্জের সীমা\n",
    "custom_labels = ['Low', 'Medium', 'High'] # কাস্টম লেবেল\n",
    "df_discretize['Score_Bins_Custom'] = pd.cut(df_discretize['Score'], bins=custom_bins, labels=custom_labels, right=False)\n",
    "print(\"\\nDataFrame after Custom Binning:\\n\", df_discretize)\n",
    "3.3. কোয়ান্টাইল ট্রান্সফরমেশন (Quantile Transformation)\n",
    "Python\n",
    "\n",
    "# একটি স্কিউড ডেটা তৈরি করি\n",
    "np.random.seed(42)\n",
    "skewed_data = np.random.exponential(scale=5, size=100).reshape(-1, 1) # এক্সপোনেনশিয়াল ডিস্ট্রিবিউশন\n",
    "df_quantile = pd.DataFrame(skewed_data, columns=['Skewed_Feature'])\n",
    "print(\"\\nOriginal Skewed Data:\\n\", df_quantile.head())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_quantile['Skewed_Feature'], kde=True)\n",
    "plt.title(\"Original Skewed Distribution\")\n",
    "\n",
    "# ইউনিফর্ম আউটপুট ডিস্ট্রিবিউশনে রূপান্তর\n",
    "qt_uniform = QuantileTransformer(output_distribution='uniform', random_state=42)\n",
    "df_quantile['Skewed_Feature_Uniform'] = qt_uniform.fit_transform(df_quantile[['Skewed_Feature']])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_quantile['Skewed_Feature_Uniform'], kde=True)\n",
    "plt.title(\"Uniform Transformed Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_quantile['Skewed_Feature'], kde=True)\n",
    "plt.title(\"Original Skewed Distribution\")\n",
    "\n",
    "# নরমাল/গাউসিয়ান আউটপুট ডিস্ট্রিবিউশনে রূপান্তর\n",
    "qt_normal = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "df_quantile['Skewed_Feature_Normal'] = qt_normal.fit_transform(df_quantile[['Skewed_Feature']])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_quantile['Skewed_Feature_Normal'], kde=True)\n",
    "plt.title(\"Normal Transformed Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDataFrame after Quantile Transformation (Uniform & Normal):\\n\", df_quantile.head())\n",
    "3.4. অ্যাট্রিবিউট কনস্ট্রাকশন / বৈশিষ্ট্য প্রকৌশল (Attribute Construction / Feature Engineering)\n",
    "Python\n",
    "\n",
    "# গাণিতিক অপারেশন ব্যবহার করে ফিচার ইঞ্জিনিয়ারিং\n",
    "df_feat_eng = pd.DataFrame({\n",
    "    'Monthly_Income': [5000, 6000, 4500],\n",
    "    'Monthly_Expense': [3000, 4000, 2000],\n",
    "    'Height_cm': [170, 165, 180],\n",
    "    'Weight_kg': [65, 58, 80]\n",
    "})\n",
    "print(\"Original DataFrame for Feature Engineering:\\n\", df_feat_eng)\n",
    "\n",
    "# 'Monthly_Savings' ফিচার তৈরি\n",
    "df_feat_eng['Monthly_Savings'] = df_feat_eng['Monthly_Income'] - df_feat_eng['Monthly_Expense']\n",
    "\n",
    "# 'BMI' ফিচার তৈরি (BMI = Weight / (Height_m)^2)\n",
    "df_feat_eng['Height_m'] = df_feat_eng['Height_cm'] / 100\n",
    "df_feat_eng['BMI'] = df_feat_eng['Weight_kg'] / (df_feat_eng['Height_m']**2)\n",
    "print(\"\\nDataFrame after creating 'Monthly_Savings' and 'BMI':\\n\", df_feat_eng)\n",
    "\n",
    "# তারিখ থেকে বৈশিষ্ট্য নিষ্কাশন\n",
    "df_date_feat = pd.DataFrame({'Transaction_Date': pd.to_datetime(['2023-01-15', '2023-02-20', '2023-03-01', '2024-06-08'])})\n",
    "print(\"\\nOriginal Date DataFrame:\\n\", df_date_feat)\n",
    "\n",
    "df_date_feat['Year'] = df_date_feat['Transaction_Date'].dt.year\n",
    "df_date_feat['Month'] = df_date_feat['Transaction_Date'].dt.month\n",
    "df_date_feat['Day_of_Week'] = df_date_feat['Transaction_Date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "df_date_feat['Is_Weekend'] = df_date_feat['Transaction_Date'].dt.dayofweek.isin([5, 6]).astype(int) # 5=Saturday, 6=Sunday\n",
    "print(\"\\nDataFrame with Date-based Features:\\n\", df_date_feat)\n",
    "4. ডেটা রিডাকশন (Data Reduction) - কোড উদাহরণ\n",
    "ডেটা সেট তৈরি:\n",
    "\n",
    "Python\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification # ডেমো ডেটা তৈরির জন্য\n",
    "\n",
    "# ফিচার সিলেকশন এবং এক্সট্রাকশনের জন্য ডেমো ডেটা তৈরি করি\n",
    "X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=3, n_repeated=2, n_classes=2, random_state=42)\n",
    "df_reduction = pd.DataFrame(X, columns=[f'Feature_{i}' for i in range(10)])\n",
    "df_reduction['Target'] = y # টার্গেট কলাম (ক্লাসিফিকেশনের জন্য)\n",
    "print(\"Original DataFrame for Data Reduction (first 5 rows):\\n\", df_reduction.head())\n",
    "print(\"Original DataFrame shape:\", df_reduction.shape)\n",
    "4.1. ফিচার নির্বাচন (Feature Selection)\n",
    "কৌশল: সিলেক্টকেবেস্ট (SelectKBest) - ফিল্টার মেথড\n",
    "\n",
    "Python\n",
    "\n",
    "# SelectKBest ব্যবহার করে সেরা 5টি ফিচার নির্বাচন\n",
    "# classification সমস্যার জন্য f_classif ব্যবহার করা হয়। regression এর জন্য f_regression\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(df_reduction.drop('Target', axis=1), df_reduction['Target'])\n",
    "\n",
    "# নির্বাচিত ফিচারের নামগুলো বের করা\n",
    "selected_features_indices = selector.get_support(indices=True)\n",
    "selected_feature_names = df_reduction.drop('Target', axis=1).columns[selected_features_indices]\n",
    "\n",
    "df_selected_features = pd.DataFrame(X_selected, columns=selected_feature_names)\n",
    "print(\"\\nDataFrame after Feature Selection (SelectKBest, top 5 features):\\n\", df_selected_features.head())\n",
    "print(\"Selected Feature Names:\", selected_feature_names.tolist())\n",
    "print(\"Selected DataFrame shape:\", df_selected_features.shape)\n",
    "4.2. ফিচার নিষ্কাশন (Feature Extraction) - PCA\n",
    "Python\n",
    "\n",
    "# PCA (Principal Component Analysis) ব্যবহার করে মাত্রিকতা হ্রাস (যেমন 2টি কম্পোনেন্ট)\n",
    "pca = PCA(n_components=2)\n",
    "# ডেটা স্কেল করে নেওয়া PCA এর আগে একটি ভালো প্র্যাকটিস\n",
    "scaled_data_for_pca = StandardScaler().fit_transform(df_reduction.drop('Target', axis=1))\n",
    "\n",
    "principal_components = pca.fit_transform(scaled_data_for_pca)\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['Principal_Component_1', 'Principal_Component_2'])\n",
    "df_pca['Target'] = df_reduction['Target'] # টার্গেট কলাম যোগ করা\n",
    "\n",
    "print(\"\\nDataFrame after PCA (2 Principal Components):\\n\", df_pca.head())\n",
    "print(\"Explained variance ratio by components:\", pca.explained_variance_ratio_)\n",
    "print(\"PCA DataFrame shape:\", df_pca.shape)\n",
    "4.3. ডেটা স্যাম্পলিং (Data Sampling)\n",
    "Python\n",
    "\n",
    "# ডেটা স্যাম্পলিং (যেমন 50% স্যাম্পল)\n",
    "df_sampled = df_reduction.sample(frac=0.5, random_state=42) # frac=0.5 মানে 50% ডেটা\n",
    "print(\"\\nDataFrame after Random Sampling (50%):\\n\", df_sampled.head())\n",
    "print(\"Sampled DataFrame shape:\", df_sampled.shape)\n",
    "\n",
    "# নির্দিষ্ট সংখ্যক স্যাম্পল\n",
    "df_sampled_n = df_reduction.sample(n=30, random_state=42) # 30টি স্যাম্পল\n",
    "print(\"\\nDataFrame after Sampling (n=30):\\n\", df_sampled_n.head())\n",
    "print(\"Sampled DataFrame (n=30) shape:\", df_sampled_n.shape)\n",
    "5. এনকোডিং ক্যাটাগরিক্যাল ডেটা (Encoding Categorical Data) - কোড উদাহরণ\n",
    "ডেটা সেট তৈরি:\n",
    "\n",
    "Python\n",
    "\n",
    "data_categorical = {'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small', 'Large'],\n",
    "                    'Color': ['Red', 'Green', 'Blue', 'Red', 'Green', 'Red'],\n",
    "                    'City': ['NYC', 'LA', 'Chicago', 'NYC', 'LA', 'Houston']}\n",
    "df_categorical = pd.DataFrame(data_categorical)\n",
    "print(\"Original Categorical DataFrame:\\n\", df_categorical)\n",
    "5.1. লেবেল এনকোডিং (Label Encoding)\n",
    "Python\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 'Size' কলামের জন্য লেবেল এনকোডিং (এখানে ক্রম আছে বলে ধরা হলো)\n",
    "le = LabelEncoder()\n",
    "df_label_encoded = df_categorical.copy()\n",
    "df_label_encoded['Size_Encoded'] = le.fit_transform(df_label_encoded['Size'])\n",
    "print(\"\\nDataFrame after Label Encoding ('Size' column):\\n\", df_label_encoded)\n",
    "print(\"Mapping for 'Size':\", list(le.classes_), \"->\", list(range(len(le.classes_))))\n",
    "5.2. ওয়ান-হট এনকোডিং (One-Hot Encoding)\n",
    "Python\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 'Color' কলামের জন্য ওয়ান-হট এনকোডিং (এখানে ক্রম নেই)\n",
    "# drop_first=True ব্যবহার করলে কলামের সংখ্যা কমে, যা মাল্টিকোলিনিয়ারিটি এড়াতে সাহায্য করে।\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first') # sparse_output=False যাতে NumPy অ্যারে দেয়\n",
    "\n",
    "# 'Color' কলামটি নতুন ডেটাফ্রেম হিসেবে এনকোড করা\n",
    "color_encoded_array = ohe.fit_transform(df_categorical[['Color']])\n",
    "color_encoded_df = pd.DataFrame(color_encoded_array, columns=ohe.get_feature_names_out(['Color']))\n",
    "\n",
    "# আসল ডেটাফ্রেমের সাথে যোগ করা\n",
    "df_one_hot_encoded = pd.concat([df_categorical, color_encoded_df], axis=1)\n",
    "print(\"\\nDataFrame after One-Hot Encoding ('Color' column):\\n\", df_one_hot_encoded)\n",
    "\n",
    "# pandas এর get_dummies ব্যবহার করে ওয়ান-হট এনকোডিং (আরো সহজ)\n",
    "# এটি একই সাথে একাধিক কলাম এনকোড করতে পারে\n",
    "df_dummies = pd.get_dummies(df_categorical, columns=['City', 'Color'], drop_first=True)\n",
    "print(\"\\nDataFrame after One-Hot Encoding using pd.get_dummies ('City', 'Color' columns):\\n\", df_dummies)\n",
    "এই কোড উদাহরণগুলি আপনাকে ডেটা প্রিপ্রসেসিংয়ের প্রতিটি ধাপের ব্যবহারিক প্রয়োগ সম্পর্কে একটি ভালো ধারণা দেবে। মনে রাখবেন, বাস্তব ডেটাসেটে আপনাকে ডেটার প্রকৃতি অনুযায়ী এই কৌশলগুলি প্রয়োগ করতে হবে এবং প্রায়শই একাধিক কৌশল একসাথে ব্যবহার করতে হবে।"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e07cf166",
   "metadata": {},
   "source": [
    "Feature scaling is a crucial preprocessing step in machine learning that transforms the values of numerical features in a dataset to a similar scale. This is important because many machine learning algorithms are sensitive to the magnitude and units of features. If features have vastly different ranges, the algorithm might implicitly give more weight to features with larger values, leading to biased results or slower convergence.\n",
    "\n",
    "Here are some of the most common feature scaling techniques:\n",
    "\n",
    "### 1. Standard Scaling (Standardization / Z-score Normalization)\n",
    "\n",
    "* **Concept:** Standard scaling transforms the data so that it has a mean of 0 and a standard deviation of 1. It assumes that the data follows a Gaussian (normal) distribution, although it can still be effective even if the data isn't perfectly normal.\n",
    "* **Formula:** For each data point $X_i$, the scaled value is calculated as:\n",
    "    $$X_{scaled} = \\frac{X_i - \\mu}{\\sigma}$$\n",
    "    where $\\mu$ is the mean of the feature values and $\\sigma$ is the standard deviation of the feature values.\n",
    "* **When to use:**\n",
    "    * Algorithms that assume normally distributed data (e.g., Linear Regression, Logistic Regression, Support Vector Machines with RBF kernel).\n",
    "    * Algorithms that calculate distances between data points (e.g., K-Nearest Neighbors, K-Means Clustering) as it ensures all features contribute equally to the distance calculation.\n",
    "    * Gradient-based optimization algorithms, as it can help them converge faster.\n",
    "* **Sensitivity to outliers:** Standard scaling is sensitive to outliers because the mean and standard deviation are heavily influenced by extreme values. Outliers can distort the scaling, causing most data points to be squished into a narrow range.\n",
    "\n",
    "### 2. Min-Max Scaling (Normalization)\n",
    "\n",
    "* **Concept:** Min-Max scaling, often simply called \"Normalization,\" rescales the data to a fixed range, typically between 0 and 1. This means the minimum value of a feature will be 0, and the maximum value will be 1, with all other values falling in between.\n",
    "* **Formula:** For each data point $X_i$, the scaled value is calculated as:\n",
    "    $$X_{scaled} = \\frac{X_i - X_{min}}{X_{max} - X_{min}}$$\n",
    "    where $X_{min}$ is the minimum value of the feature and $X_{max}$ is the maximum value of the feature.\n",
    "* **When to use:**\n",
    "    * Algorithms that require feature values to be within a specific bounded range (e.g., Neural Networks, image processing where pixel values are often between 0 and 255).\n",
    "    * When the data distribution is not Gaussian and the range is meaningful.\n",
    "    * When you want to preserve the original distribution shape.\n",
    "* **Sensitivity to outliers:** Min-Max scaling is highly sensitive to outliers. A single extreme outlier can compress the majority of the data into a very small range, making it difficult for the model to learn from the variations in the non-outlier data.\n",
    "\n",
    "### 3. Robust Scaling\n",
    "\n",
    "* **Concept:** Robust scaling is designed to be robust to outliers. Instead of using the mean and standard deviation, it uses the median and the interquartile range (IQR) to scale the data. The IQR is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) of the data.\n",
    "* **Formula:** For each data point $X_i$, the scaled value is calculated as:\n",
    "    $$X_{scaled} = \\frac{X_i - \\text{Median}}{\\text{IQR}}$$\n",
    "    where Median is the median of the feature values, and IQR is the interquartile range (Q3 - Q1).\n",
    "* **When to use:**\n",
    "    * When your dataset contains many outliers.\n",
    "    * When you want to maintain the relative distances between non-outlier data points.\n",
    "    * For algorithms sensitive to extreme values.\n",
    "* **Advantages:** It is less affected by outliers compared to Standard Scaling and Min-Max Scaling, and it doesn't make assumptions about the data's distribution.\n",
    "* **Disadvantages:** It can be less interpretable than Min-Max scaling and may not perform as well if the data is highly skewed.\n",
    "\n",
    "### Other Feature Scaling Techniques:\n",
    "\n",
    "While Standard, Min-Max, and Robust scaling are the most common, there are other specialized techniques:\n",
    "\n",
    "* **MaxAbs Scaling:** Scales each feature by its maximum absolute value. This method doesn't shift or center the data, so it preserves sparsity (useful for sparse matrices where many values are zero). The scaled values will be in the range [-1, 1].\n",
    "* **Normalization (Unit Vector Scaling):** This technique scales each sample (row) to have a unit norm (a length of 1). It's useful when the magnitude of the vector is not as important as its direction, such as in text classification or when working with image features.\n",
    "* **Log Transformation:** Applies a logarithmic transformation to skewed data. This can help reduce the impact of large values and make the data distribution more symmetrical, often closer to a normal distribution.\n",
    "* **Power Transformer Scaler (Box-Cox, Yeo-Johnson):** These are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. They can be very effective for handling skewed data.\n",
    "* **Quantile Transformer Scaler:** Transforms features using quantile information. It maps the data to a uniform or normal distribution, spreading out frequent values and reducing the impact of marginal outliers.\n",
    "\n",
    "### Why is Feature Scaling Important?\n",
    "\n",
    "* **Algorithm Performance:** Many machine learning algorithms (especially distance-based or gradient-based ones) perform better and converge faster when features are on a similar scale.\n",
    "* **Equal Contribution:** It ensures that all features contribute equally to the model's learning process, preventing features with larger numerical ranges from dominating the learning.\n",
    "* **Avoid Bias:** Without scaling, features with larger values might be perceived as more important by the model, even if they are not.\n",
    "* **Interpretability:** In some cases, scaling can make the model's coefficients or learned weights more interpretable, as they reflect the true contribution of each feature rather than being skewed by scale differences.\n",
    "\n",
    "Choosing the right scaling technique depends on the characteristics of your data (e.g., presence of outliers, distribution shape) and the specific machine learning algorithm you plan to use. It's often a good practice to experiment with different scaling methods to see which one yields the best performance for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6515a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24786ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "    \"name\":[\"siz\",\"miz\",\"kiz\",\"liz\"],\n",
    "    \"age\":[20,30,40,50],\n",
    "    \"hight\":[1.70,1.80,1.90,2.00],\n",
    "    \"weight\":[60,70,80,90]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2293322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf30ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name       age     hight    weight\n",
      "0  siz -1.341641 -1.341641 -1.341641\n",
      "1  miz -0.447214 -0.447214 -0.447214\n",
      "2  kiz  0.447214  0.447214  0.447214\n",
      "3  liz  1.341641  1.341641  1.341641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler to the DataFrame   \n",
    "scaler.fit_transform(df[['age', 'hight', 'weight']])\n",
    "# Transform the DataFrame\n",
    "df[['age', 'hight', 'weight']] = scaler.transform(df[['age', 'hight', 'weight']])\n",
    "# Print the transformed DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c83276f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name       age     hight    weight\n",
      "0  siz  0.000000  0.000000  0.000000\n",
      "1  miz  0.333333  0.333333  0.333333\n",
      "2  kiz  0.666667  0.666667  0.666667\n",
      "3  liz  1.000000  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "# Fit the scaler to the DataFrame\n",
    "min_max_scaler.fit_transform(df[['age', 'hight', 'weight']])\n",
    "# Transform the DataFrame\n",
    "df[['age', 'hight', 'weight']] = min_max_scaler.transform(df[['age', 'hight', 'weight']])\n",
    "# Print the transformed DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a42f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name       age     hight    weight\n",
      "0  siz -1.000000 -1.000000 -1.000000\n",
      "1  miz -0.333333 -0.333333 -0.333333\n",
      "2  kiz  0.333333  0.333333  0.333333\n",
      "3  liz  1.000000  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "# Fit the scaler to the DataFrame\n",
    "scaler.fit_transform(df[['age', 'hight', 'weight']])\n",
    "# Transform the DataFrame\n",
    "df[['age', 'hight', 'weight']] = scaler.transform(df[['age', 'hight', 'weight']])   \n",
    "# Print the transformed DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
